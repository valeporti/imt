{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nicolas Sholten  \n",
    "Valentin Portillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBNJ-dB-yyhJ"
   },
   "source": [
    "<span style=\"font-size: 30px\">**Pix2Pix**</span>\n",
    "\n",
    "<span style=\"font-size: 20px\">**Image to image translation with Conditional Adversarial Networks**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAu4T43ky3GU"
   },
   "source": [
    "## Context\n",
    "\n",
    "<span style=\"font-size: 18px\">Pix2Pix algorithm works in the context of **Generative Models** on the **Machine Learning** terrain. More specifically, in **Deep Learning**. This algorithm uses the principles of **Generative Adversarial Networks (GANs)**, meaning that a discriminator and generator are present in the arquitecture trying to correct or fool one another respectively. **The generator** is built with an \"auto-encoder architecture\", meaning that it would try to capture the intrinsec distributions needed to satisfy the goal representation from the input representation. In the contrary, **the discriminator** will evaluate the output of the generator and calssify it as fake or real.</span>\n",
    "\n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-DL9EaT5Oc8"
   },
   "source": [
    "## Problem\n",
    "\n",
    "<span style=\"font-size: 18px\">Pix2Pix answers to the need of **translating the input image to the corresponding output image**. Meaning that as an input you can have a representation of an image and at the output you would have the image it represents. The examples are variate: </span>\n",
    "\n",
    "<p></p>\n",
    "<span style=\"font-size: 18px\">\n",
    "    <ul>\n",
    "        <li>Semantic labels ↔ photothese</li>\n",
    "        <li>Architectural labels → photo</li>\n",
    "        <li>Map ↔ aerial photo</li>\n",
    "        <li>BW → color photos</li>\n",
    "        <li>Edges → photo</li>\n",
    "        <li>Sketch → photo: tests edges → photo models on human-drawn sketches</li>\n",
    "        <li>Day → night</li>\n",
    "        <li>Thermal → color photos</li>\n",
    "        <li>Photo with missing pixels → inpainted photo</li>\n",
    "    </ul>   \n",
    "</span>\n",
    "\n",
    "![alt text](https://phillipi.github.io/pix2pix/images/teaser_v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nD7YFM_5Rox"
   },
   "source": [
    "## Formulation\n",
    "\n",
    "<span style=\"font-size: 18px\">**Objective function:**</span>\n",
    "\n",
    "\\begin{equation}\n",
    "L_{cGAN}(G, D) = E_{x,y}[logD(x,y)]+E_{x,z}[log(1−D(x,G(x,z))]\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"font-size: 18px\">-> *G tries to minimize the objective function against an adversarial D that tries to maximize it*</span>\n",
    "\n",
    "\\begin{equation}\n",
    "G^∗ = arg min_{G}max_{D}L_{cGAN}(G,D)\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"font-size: 18px\">**Generator Loss:**</span>\n",
    "\n",
    "\\begin{equation}\n",
    "L_{L1}(G) =E_{x,y,z}[‖y−G(x,z)‖]\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"font-size: 18px\">then, the final objective is:</span>\n",
    "\n",
    "\\begin{equation}\n",
    "G^∗ = arg min_{G}max_{D}L_{cGAN}(G,D) + λL_{L1}(G)\n",
    "\\end{equation}\n",
    "\n",
    "> where  \n",
    "        *  z - noise\n",
    "        *  y - output\n",
    "        *  x - condition (conditional image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQPdNPib5WdY"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "<span style=\"font-size: 18px\">In order to perform these tasks, for the proposed tool, more specifically than a GAN, there are some considerations or adaptations done to this architecture. As a first thing, the GAN is a **conditional GAN (cGAN)** which means that as an input, the GAN not only expects noise but a type. This type could be a label such as \"drawing\" or in our case, a kind of image such as a handdrawed purse and as an output we expect an image of a purse. Additionally, they use a helper feature to do the downsampling and upsampling of the image: they propose a \"bridge\" between equivalent levels of downsampling and upsampling in order to help correct the possible errors inherent of the information loss done by this operation. This \"bridge\" is called **\"skip connections\"** and the U-Net is the architecture that proposes this type of connection. Also, another tweak done to the GAN initial arquitecture is to use **L1 loss type** for the Generator since it is \"less strong\" than a L2 (squared error) it prevents the pixels to turn blurry, even though, for the discriminator, it still uses a L2 loss type. Furthermore, a key part on doing the downsampling and upsampling is to apply **normalization** after each activation function output in order to avoid skyreocketed values and maintian the values controlled between 0 and 1. Finally, at the discriminator level, the fake/real note is given by an ensemble of notes, it's not just one, so the last layer is divided into patches, each evaluating a part of the resulting last layer, so penalizing at the scale of patches. This type of structure models the image as a Markov random field which assumes independence between pixels separated by more than a patch diameter causing to model only high frequency structure correcting mainly low frequencies. This architecture is called **PatchGAN**.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 18px\">**cGan**</span>\n",
    "> Difference with GAN -> we use a condition as an input (label, image ...)\n",
    "\n",
    "<img style=\"width: 500px\" src=\"http://nooverfit.com/wp/wp-content/uploads/2017/10/Screenshot-from-2017-10-07-120039.png\">\n",
    "\n",
    "<span style=\"font-size: 18px\">**U-net**</span>\n",
    "> On Generator, non exact example, there's an extra convolutional layer and missing normalization layer. Besides, sizes are different\n",
    "\n",
    "<img style=\"width: 800px\" src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fblog.qure.ai%2Fassets%2Fimages%2Fsegmentation-review%2Funet.png&f=1&nofb=1\">\n",
    "\n",
    "<span style=\"font-size: 18px\">**L1 loss**</span>\n",
    "> Comparison between L1 (absolute value) and L2 (squared value) losses\n",
    "\n",
    "<img style=\"width: 400px\" src=\"https://t1.daumcdn.net/cfile/tistory/999BFC475AE189661D\">\n",
    "\n",
    "<span style=\"font-size: 18px\">**Patch Gan**</span>\n",
    "> Example\n",
    "\n",
    "<img style=\"width: 350px\" src=\"https://paper-attachments.dropbox.com/s_84D9D849F786EC83B26BF2A0F74F0C33230682E8BA1D41AD8C3F3D770D23236A_1566269789757_pg.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ke8wfKV_5bbn"
   },
   "source": [
    "## Results\n",
    "\n",
    "<span style=\"font-size: 18px\">After running the algorithm on the normal settings for a few hours, we obtained results close to what we were expecting, but not as impressive as we thought. We noticed 2 different outcomes:\n",
    "\n",
    "<span style=\"font-size: 18px\">The first one was surprising:\n",
    "\n",
    "<span style=\"font-size: 18px\">Indeed, after 120 iterations, each one on 500 image of facade in labels, we obtained an accuracy of the discriminator of approximately 50% which means that the generator was able to fool the disciriminator half the time. It was therefore improving a lot and we can see that in the images we obtained in results.\n",
    "\n",
    "<img src=\"./images/facades (10 epochs, 64 filters, 3'''56''56')/120_0.png\">\n",
    "\n",
    "<span style=\"font-size: 18px\">However, after approximatey 150 iterations, the discriminator's accuracy got closer to 100% and was always equal to 100% after 170 iterations. This means that the generator was not able to fool the discriminator anymore and was therefore not improving at all. This can be seen in the results, which are not really better than after 120 iterations.\n",
    "\n",
    "<img src=\"./images/facades (10 epochs, 64 filters, 3'''56''56')/170_0.png\">\n",
    "\n",
    "<span style=\"font-size: 18px\">The second outcome was more logical:\n",
    "\n",
    "<span style=\"font-size: 18px\">Even after 200 iterations, the discriminator's accuracy stayed close to 50% (a bit higher but not that much). The generator is then still improving and can fool the discriminator half the time. We can see in results we obtained, as the generated photos seem a bit closer to reality.\n",
    "\n",
    "<img src=\"./images/facadesV2 (50percent) (epochs=200, batch_size=20, sample_interval=200)/195_0.png\">\n",
    "\n",
    "<span style=\"font-size: 18px\">We then tried to change the number of filters (with no changes in the model and 10 iterations)\n",
    "\n",
    "<img src=\"./images/facades (10 epochs, 32 filters, 5''48')/9_200.png\">\n",
    "<center>Run with 32 filters: 5 minutes 48 seconds <br/> Total parameters: 11,166,244</center>\n",
    "\n",
    "<img src=\"./images/facades (10 epochs, 64 filters, 12''15')/9_200.png\">\n",
    "<center>Run with 64 filters: 12 minutes 15 seconds <br/> Total parameters: 44,614,724</center>\n",
    "\n",
    "<img src=\"./images/facades (10 epochs, 128 filters, 42''13')/9_200.png\">\n",
    "<center>Run with 128 filters: 42 minutes 13 seconds <br/> Total parameters: 178,358,404</center>\n",
    "\n",
    "<span style=\"font-size: 18px\">We can see that increasing the number of filters slighty improves the quality of the image generated but it also takes a lot more time as they are a lot more parameters. It's a compromise between time and quality. We used 64 filters for our future tests, as it seemed to be a good compromise.\n",
    "\n",
    "<br/>\n",
    "<span style=\"font-size: 18px\">We then tried to change the architecture of the model (with 64 filters and 10 iterations)\n",
    "\n",
    "<img src=\"./images/Architecture change/3_facades (10 epochs, 64 filters, minusd7, 4''25', Total params: 32,026,692)/9_0.png\">\n",
    "<center>Run with a layer removed: 4 minutes 25 seconds <br/> Total parameters: 32,026,692</center>\n",
    "<img src=\"./images/Architecture change/2_facades (10 epochs, 64 filters,no architecture change, 4''30', Total params: 44,614,724)/9_0.png\">\n",
    "<center>Run with no layer removed: 4 minutes 30 seconds <br/> Total parameters: 44,614,724</center>\n",
    "\n",
    "<img src=\"./images/Architecture change/1_facades (10 epochs, 64 filters, plusd8, 4''37', Total params: 57,202,756)/9_0.png\">\n",
    "<center>Run with a layer added: 4 minutes 37 seconds <br/> Total parameters: 57,202,756</center>\n",
    "\n",
    "<span style=\"font-size: 18px\">Changing the number of layers doesn't really impact the algorithm. Indeed it doesn't add a lot of parameters. The results are just a tiny bit better but this is not a very effective way to improve the efficiency of the algorithm.\n",
    "\n",
    "<br/>\n",
    "<span style=\"font-size: 18px\">To conclude our tests, it is noticeable that the algorithm is effectiv. It relies on a complex balance between quality and number of parameters. However, this relation seems like a logarithm. It is necessary to increase the number of parameters a lot (and therefore the time spent) to realy improve quality or the results.\n",
    "<br/>Moreover, it's interresting to notice that it failed one time, which shows that it isn't flawless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gkO0gfP5ig1"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "<span style=\"font-size: 18px\">The results obtained during our tests were conclusive and it's clear that this algorithm is one of the correct approaches for image-to-image translation.\n",
    "This algorithm, which wasn't even thought of 5 years ago is great step. It needs improvement and corrections that could make it faster and reliable every time but it's future applications could be huge.\n",
    "However, this system has limits as it can't be used in the same way for all it's application. Indeed, it's very different to colorize black and with photo than to create a scene only using labels[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros, *\"Image-to-Image Translation with Conditional Adversarial Networks\"*, Berkeley AI Research (BAIR) Laboratory, UC Berkeley\n",
    "\n",
    "[2] Richard Zhang, Phillip Isola, Alexei A. Efros, *Colorful Image Colorization*, University of California, Berkeley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes\n",
    "\n",
    "<img src=\"./datasets/Modèle discriminateur.png\">\n",
    "<center>Modèle du discriminateur\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<img src=\"./datasets/Modèle générateur.png\">\n",
    "<center>Modèle du générateur\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pix2pix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
