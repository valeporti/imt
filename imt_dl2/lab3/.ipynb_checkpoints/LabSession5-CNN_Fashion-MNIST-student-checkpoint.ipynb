{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE Deep learning -- Lab session 5\n",
    "Pierre-Henri Conze, Fran√ßois Rousseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives: perform classification on Fashion-MNIST dataset using multiple layer perceptrons, convolutional neural networks, data augmentation and transfer learning to obtain the best classification results as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download and read the Fashion-MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data management and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Fashion-MNIST is a dataset consisting of a training set of A examples and a test set of B examples. Each example is a CxC grayscale image, associated with a label from D classes. What are the values for A, B, C and D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 60000, B: 10000, CxC: (28, 28), D: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# to do\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "img_shape = ( x_train.shape[1], x_train.shape[1])\n",
    "classes = np.unique(y_train)\n",
    "print(f'A: {train_size}, B: {test_size}, CxC: {img_shape}, D: {classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training/test sample is assigned to one of the following labels: \n",
    "0\tT-shirt\n",
    "1\tTrouser\n",
    "2\tPullover\n",
    "3\tDress\n",
    "4\tCoat\n",
    "5\tSandal\n",
    "6\tShirt\n",
    "7\tSneaker\n",
    "8\tBag\n",
    "9\tAnkle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "class_names = ['tshirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Visualize one example per class among the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "def visualizeExample(n, x):\n",
    "    plt.imshow(x[n].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "    plt.clf(); plt.close();\n",
    "    \n",
    "visualizeExample(1, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Multi-layer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Define a simple Keras multi-layer perceptron (MLP) architecture using Sequential API and containing 3 dense layers: 2 dense layers with 512 units and ReLU activation followed by 1 dense layer for softmax regression. Use your model to get first classification results on Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build 4D tensors\n",
    "x_train = x_train.reshape(train_size, img_shape[0], img_shape[1], 1)\n",
    "x_test = x_test.reshape(test_size, img_shape[0], img_shape[1], 1)\n",
    "# data normalization\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "# convert class vectors to binary class matrices\n",
    "z_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "z_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/v18porti/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2a529309e331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmlp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmlp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmlp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/v18porti/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1252\u001b[0;31m                 \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m'Build the model first by calling build() '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m'or calling fit() with some data. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "
     ]
    }
   ],
   "source": [
    "mlp_1 = Sequential()\n",
    "\n",
    "mlp_1.add(Dense(units=128, activation='relu'))\n",
    "mlp_1.add(Dense(units=128, activation='relu'))\n",
    "mlp_1.add(Dense(units=num_classes, activation='softmax'))\n",
    "mlp_1.summary()\n",
    "mlp_1.compile(RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3a74a22b747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist_mlp_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z_train' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "hist_mlp_1=mlp_1.fit(x=x_train, y=z_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(x_test, z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Define a generic function evaluate_model() able to:\n",
    "- plot the evolution of accuracy for both training and testing data with respect to the epochs\n",
    "- compute final test loss and accuracy. \n",
    "\n",
    "Use this function to assess the performance of the previously defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model,...):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate_model(mlp_1,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Define a deeper multi-layer perceptron (MLP) architecture to outperform the previously obtained classification results. \n",
    "Use evaluate_model() to provide the evaluation and comment the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_2 = Sequential()\n",
    "# mlp_2.add(...)\n",
    "# to do\n",
    "# evaluate_model(mlp_2,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Define a simple Keras convolutional neural network (CNN) architecture using Sequential API and containing the following layers : 1 convolution layer using Conv2D and 32 filters, 1 max-polling layer using MaxPooling2D, 2 dense layers using ReLU and softmax activations respectively. Use this model to get classification results (use evaluate_model() as previously) and compared them to MLP results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_1 = Sequential()\n",
    "# cnn_1.add(...)\n",
    "# cnn_1.compile(...)\n",
    "# cnn_1.summary()\n",
    "# hist_cnn_1 = cnn_1.fit(...)\n",
    "# evaluate_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) As for MLP, define a deeper CNN architecture to outperform the previously obtained classification results. What are your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_2 = Sequential()\n",
    "# cnn_2.add(...)\n",
    "# to do \n",
    "# evaluate_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Visualize wrongly predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Create a function visualize_wrongly_predictions() able to show wrong prediction for each class for a given model as input. Use this function for each of the 4 previouly trained models (2 MLP, 2 CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualize_wrongly_predictions(model,...):\n",
    "    # to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_wrongly_predictions(mlp_1,...)\n",
    "# visualize_wrongly_predictions(mlp_2,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_wrongly_predictions(cnn_1,...)\n",
    "# visualize_wrongly_predictions(cnn_2,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) Using the code given below, create a function visualize_confusion_matrix() able to display the confusion matrix  for a given model as input. Use this function for each of the 4 previouly trained models (2 MLP, 2 CNN). What is the most common confusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualize_confusion_matrix(model, ...):\n",
    "    # to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_confusion_matrix(mlp_1, ...)\n",
    "# visualize_confusion_matrix(mlp_2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_confusion_matrix(cnn_1, ...)\n",
    "# visualize_confusion_matrix(cnn_2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common confusion: #todo\n",
    "\n",
    "Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Based on the documentation (https://keras.io/preprocessing/image/), train one of your CNN architecture using data augmentation and conclude on the used of augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=8,\n",
    "    width_shift_range=0.08,\n",
    "    height_shift_range=0.08,\n",
    "    shear_range=0.3, \n",
    "    zoom_range=0.08)\n",
    "\n",
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 - Transfer learning from VGG16 trained on ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.\n",
    "\n",
    "In this part, you will do fine tuning on a pre-trained network. Fine-tuning consists in starting from a trained network, then re-training it on a new dataset using very small weight updates. To do so, you need to load a pre-trained model, add a dense classifier to compute the output, and then to freeze the weights of the pre-trained model (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11) Exploit an architecture already trained on ImageNet (https://keras.io/applications) to improve the classification results on Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from skimage.transform import resize\n",
    "\n",
    "# to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12) Challenge: the first 3 teams in terms of overall test accuracy results (whatever the methodology used) will obtain bonus points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy reached: #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: #todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
