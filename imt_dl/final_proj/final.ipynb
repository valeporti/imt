{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import initializers , preprocessing, utils\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}\n",
      "\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Alphabet\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "numbers = \"0123456789\"\n",
    "other_char = \",;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\" # original: -,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}, changed: -,;.!?:’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\n",
    "new_line = \"\\n\"\n",
    "\n",
    "alphabet = letters + numbers + other_char + new_line\n",
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "3600000\n"
     ]
    }
   ],
   "source": [
    "# type of text\n",
    "# __label__2 This is my first PDA/Organizer: I purchased this about 4 months ago and it really is easy to use especially if you are familiar with Microsoft Word and Excel. I use it to copy files from my PC that I want to have handy. The only thing I wish it had is a backlight. But that hasn't come into play too often. I usually have to charge it every 2 or 3 days. I recommend this product if you are not worried about having a lot accessories to go with it, because they are not that many available.\n",
    "# __label__1 Piece of Crap: I have ordered thousands of items in my lifetime, and bar none, this is the biggest piece of crap I have ever received. It is supposed to come completely put together, but when it arrived, there were no less than 10 little parts that had come off the screen assembly and one screen that was loose and all of its parts dangling. The first thing that happenned is that I cut myself on one of the screens because the protective side had come off. I was bleeding for quite some time. The second thing I noticed was the extremely poor quality of the material and assembly. I am very handy by nature, but even I had to give up trying to put this piece of junk back together again. Plus it was not worth it, even if I had put it together again, all I would have then had was a piece of crap fireplace screen.\n",
    "# data from  https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "\n",
    "dir_name = './data/'\n",
    "test_file_name = 'test.ft.txt'\n",
    "train_file_name = 'train.ft.txt'\n",
    "\n",
    "\n",
    "def getListOfLabelsAndLinesFromFile(file_name): # label 1 = 0, label 2 = 1\n",
    "    \n",
    "    labels = []\n",
    "    lines = []\n",
    "    total_labels = []\n",
    "    count = 0\n",
    "    file = open(file_name, 'r')\n",
    "    for line in file:\n",
    "        match = re.match('(__label__([0-9])) (.*)', line)\n",
    "        if match:\n",
    "            label = int(match.group(2)) - 1 # to make the label start in 0 (ZERO)\n",
    "            text = match.group(3)\n",
    "            if not label in total_labels: total_labels.append(label)\n",
    "            labels.append(label)\n",
    "            lines.append(text)\n",
    "        else:\n",
    "            print('---- ERROR ----')\n",
    "        count += 1\n",
    "    file.close()\n",
    "    return { 'labels': labels, 'lines': lines, 'total_labels': total_labels, 'total_lines': count }\n",
    "\n",
    "train_data_info = getListOfLabelsAndLinesFromFile(dir_name + train_file_name)\n",
    "test_data_info = getListOfLabelsAndLinesFromFile(dir_name + test_file_name)\n",
    "\n",
    "print(test_data_info['total_lines'])\n",
    "print(train_data_info['total_lines'])\n",
    "# ratio 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Embedding\n",
    "max_len_text = 1014\n",
    "max_features_for_embedding = len(alphabet)\n",
    "output_from_embedding = 128\n",
    "\n",
    "# Large Model\n",
    "initializer_large = { 'mean': 0.0, 'stddev': 0.02}\n",
    "activation_conv = 'relu'\n",
    "conv_output_dim = 256\n",
    "pool_size = 3\n",
    "kernel_size_7 = 7 # first_WITH_maxpool\n",
    "kernel_size_3 = 3 # intermediate_NO_maxpool\n",
    "\n",
    "# Small model\n",
    "initializer_small = { 'mean': 0.0, 'stddev': 0.05}\n",
    "activation_dens = 'relu'\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1014, 128)         8832      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1008, 256)         229632    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 336, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 330, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 106, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 104, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 102, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,450,498\n",
      "Trainable params: 11,450,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "# https://medium.com/@romannempyre/sentiment-analysis-using-1d-convolutional-neural-networks-part-1-f8b6316489a2\n",
    "# https://missinglink.ai/guides/deep-learning-frameworks/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
    "# https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=5c4a324fc922\n",
    "# https://medium.com/@bramblexu/character-level-cnn-with-keras-50391c3adf33\n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# AS the one hot approach, leave us with a very sparse and high dimensional matrix, we apply embedding\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "# Embedding ( \n",
    "#    max_features/input_dim = we got just 70 different types of inputs / Size of the vocabulary, \n",
    "#    output_dim = learn N dimensional embeddings for each of the input_dim words/characters,\n",
    "#    input_length needed to flatten = cut the words/characters of each element to that lenght so the max\n",
    "#                                     qty of elements in each phrase will be that one\n",
    "# )    \n",
    "model.add(layers.Embedding(max_features_for_embedding, output_from_embedding, input_length=max_len_text))\n",
    "# After the Embedding layer,\n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "# Output: (batch dimension, input_length, output_dim)\n",
    "\n",
    "# Convolutional Layer 1D\n",
    "# In text \n",
    "# Conv1D(\n",
    "#    filters = dimensionality of the output space\n",
    "#    kernel_size = window_size\n",
    "#    stride = position jumps of the window (defaults to 1)\n",
    "# )\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv, kernel_initializer=initializer_large_model))\n",
    "\n",
    "# Max Pooling 1D\n",
    "# MaxPool1D(\n",
    "#    pool_size = window size\n",
    "#    strides = by default equals pool_size, so each window doesn't overlap\n",
    "# )\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "\n",
    "# Second part, full connected layers\n",
    "initializer_small_model = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation=activation_dens, kernel_initializer=initializer_small_model))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(1024, activation=activation_dens))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "# Last layer, according to problem to solve\n",
    "# (just have 2 classes)\n",
    "# But, it has to be though a Dense(2,...) , with Dense(1,...) not working so neither do activation \"sigmoid\"\n",
    "model.add(layers.Dense(2, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer and loss function and metrics to return \n",
    "# https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlphabetDict(alpha):\n",
    "    d = {}; c = 1\n",
    "    for a in alpha: d[a] = c; c += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndClasses(tokenizer, info, alpha, max_len_text, batch_len = None, from_i = 0):\n",
    "        \n",
    "    lines = info['lines']\n",
    "    labels = info['labels']\n",
    "    if batch_len != None: \n",
    "        lines = lines[from_i:(from_i + batch_len)]\n",
    "        labels = labels[from_i:(from_i + batch_len)]\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    \"\"\"EX: [[15, 9, 2, 5, 3, 1, 13, 12, 28, 1, 14, 17 ...\"\"\" # Each, number represents a letter in the alphabet\n",
    "    data = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len_text, padding='post')\n",
    "    # Data of shape (quantity of info (sentences), max_len_text)\n",
    "    \"\"\"EX: array([[15,  9,  2, ...,  0,  0,  0],\n",
    "       [ 6,  8,  2, ...,  0,  0,  0],\n",
    "       [21,  5,  3, ...,  0,  0,  0]], dtype=int32)\"\"\"  \n",
    "    \n",
    "    classes = utils.to_categorical(labels, num_classes=len(info['total_labels']))\n",
    "    \"\"\"EX: array([[0., 1.],\n",
    "       [0., 1.],\n",
    "       [1., 0.]], dtype=float32)\"\"\"\n",
    "    \n",
    "    return {'data': data, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_arr = []\n",
    "def runByBatches(model, train_info, test_info, alpha, max_len_text, batch_size, test_batch_size, until = None):\n",
    "    \n",
    "    # Preprocess Data \n",
    "    tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=alpha) # , oov_token= ?  \n",
    "    tokenizer.word_index = getAlphabetDict(alpha)\n",
    "    \"\"\"EX: {'a': 1,\n",
    "         'b': 2,\n",
    "         'c': 3 ... \"\"\"\n",
    "    \n",
    "    # After each epoch , the test data is validated\n",
    "    # test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text)\n",
    "    test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text, test_batch_size,0)\n",
    "        \n",
    "    total_train_len = train_info['total_lines']\n",
    "    total_tst_len = test_info['total_lines']\n",
    "    count = count_tst = 0\n",
    "    \n",
    "    while count < total_train_len and (until != None and count < until):\n",
    "        \n",
    "        # Get elements\n",
    "        size = batch_size if batch_size < (total_train_len - count) else total_train_len - count\n",
    "        size_tst = test_batch_size if test_batch_size < (total_tst_len - count_tst) else total_tst_len - count_tst\n",
    "        train_data = getDataAndClasses(tokenizer, train_info, alpha, max_len_text, size, count)\n",
    "        #test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text, size_tst, count_tst)\n",
    "        print(f'-----  doing from trining/test: {count} - {size + count}') # / {count_tst} - {size_tst}')\n",
    "        count += batch_size   \n",
    "        count_tst += test_batch_size\n",
    "        \n",
    "        history = model.fit(train_data['data'], \n",
    "                            train_data['classes'],\n",
    "                            validation_data=(test_data['data'], test_data['classes']),\n",
    "                            epochs=4, # 5000\n",
    "                            batch_size=128,\n",
    "                            verbose=2\n",
    "                           )\n",
    "        hist_arr.append(history.history)\n",
    "        \n",
    "        train_data = None\n",
    "        #test_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWindowOfBatchInArray(a, from_i, to_i):\n",
    "    tot_len = len(a)\n",
    "    until_pos = from_i + to_i\n",
    "    remaining = 0\n",
    "    if until_pos > tot_len: \n",
    "        remaining = until_pos - tot_len \n",
    "        return a[from_i:(tot_len + 1)]+a[0:remaining]\n",
    "    else:\n",
    "        return a[from_i:to_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  doing from trining/test: 0 - 40000\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 402s - loss: 0.6303 - acc: 0.6198 - val_loss: 0.4988 - val_acc: 0.7622\n",
      "Epoch 2/4\n",
      " - 362s - loss: 0.4479 - acc: 0.7941 - val_loss: 0.4718 - val_acc: 0.7666\n",
      "Epoch 3/4\n",
      " - 377s - loss: 0.3468 - acc: 0.8493 - val_loss: 0.3554 - val_acc: 0.8433\n",
      "Epoch 4/4\n",
      " - 368s - loss: 0.2731 - acc: 0.8875 - val_loss: 0.3206 - val_acc: 0.8631\n",
      "-----  doing from trining/test: 40000 - 80000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 372s - loss: 0.3139 - acc: 0.8672 - val_loss: 0.3053 - val_acc: 0.8682\n",
      "Epoch 2/4\n",
      " - 387s - loss: 0.2446 - acc: 0.9004 - val_loss: 0.3084 - val_acc: 0.8773\n",
      "Epoch 3/4\n",
      " - 372s - loss: 0.1932 - acc: 0.9241 - val_loss: 0.3088 - val_acc: 0.8762\n",
      "Epoch 4/4\n",
      " - 382s - loss: 0.1440 - acc: 0.9444 - val_loss: 0.3359 - val_acc: 0.8724\n",
      "-----  doing from trining/test: 80000 - 120000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 354s - loss: 0.2780 - acc: 0.8852 - val_loss: 0.2743 - val_acc: 0.8846\n",
      "Epoch 2/4\n",
      " - 349s - loss: 0.1964 - acc: 0.9234 - val_loss: 0.2846 - val_acc: 0.8868\n",
      "Epoch 3/4\n",
      " - 390s - loss: 0.1364 - acc: 0.9491 - val_loss: 0.3371 - val_acc: 0.8742\n",
      "Epoch 4/4\n",
      " - 351s - loss: 0.0974 - acc: 0.9628 - val_loss: 0.3708 - val_acc: 0.8835\n",
      "-----  doing from trining/test: 120000 - 160000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 362s - loss: 0.2718 - acc: 0.8883 - val_loss: 0.2490 - val_acc: 0.8980\n",
      "Epoch 2/4\n",
      " - 360s - loss: 0.1879 - acc: 0.9259 - val_loss: 0.2801 - val_acc: 0.8908\n",
      "Epoch 3/4\n",
      " - 371s - loss: 0.1237 - acc: 0.9533 - val_loss: 0.3602 - val_acc: 0.8744\n",
      "Epoch 4/4\n",
      " - 354s - loss: 0.0853 - acc: 0.9688 - val_loss: 0.3747 - val_acc: 0.8891\n",
      "-----  doing from trining/test: 160000 - 200000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 358s - loss: 0.2636 - acc: 0.8922 - val_loss: 0.2664 - val_acc: 0.8943\n",
      "Epoch 2/4\n",
      " - 346s - loss: 0.1713 - acc: 0.9347 - val_loss: 0.2713 - val_acc: 0.8952\n",
      "Epoch 3/4\n",
      " - 373s - loss: 0.1099 - acc: 0.9599 - val_loss: 0.3377 - val_acc: 0.8858\n",
      "Epoch 4/4\n",
      " - 354s - loss: 0.0765 - acc: 0.9723 - val_loss: 0.3635 - val_acc: 0.8900\n",
      "-----  doing from trining/test: 200000 - 240000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 351s - loss: 0.2509 - acc: 0.8972 - val_loss: 0.2472 - val_acc: 0.8990\n",
      "Epoch 2/4\n",
      " - 347s - loss: 0.1607 - acc: 0.9392 - val_loss: 0.2738 - val_acc: 0.8992\n",
      "Epoch 3/4\n",
      " - 349s - loss: 0.1005 - acc: 0.9625 - val_loss: 0.2841 - val_acc: 0.8898\n",
      "Epoch 4/4\n",
      " - 356s - loss: 0.0700 - acc: 0.9748 - val_loss: 0.3448 - val_acc: 0.8901\n",
      "-----  doing from trining/test: 240000 - 280000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 358s - loss: 0.2506 - acc: 0.9010 - val_loss: 0.2466 - val_acc: 0.8992\n",
      "Epoch 2/4\n",
      " - 359s - loss: 0.1645 - acc: 0.9374 - val_loss: 0.2712 - val_acc: 0.9012\n",
      "Epoch 3/4\n",
      " - 354s - loss: 0.0982 - acc: 0.9648 - val_loss: 0.3171 - val_acc: 0.8907\n",
      "Epoch 4/4\n",
      " - 353s - loss: 0.0721 - acc: 0.9737 - val_loss: 0.3998 - val_acc: 0.8932\n",
      "-----  doing from trining/test: 280000 - 320000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 357s - loss: 0.2562 - acc: 0.8976 - val_loss: 0.2371 - val_acc: 0.9056\n",
      "Epoch 2/4\n",
      " - 356s - loss: 0.1636 - acc: 0.9393 - val_loss: 0.2630 - val_acc: 0.8987\n",
      "Epoch 3/4\n",
      " - 348s - loss: 0.0993 - acc: 0.9640 - val_loss: 0.2944 - val_acc: 0.8923\n",
      "Epoch 4/4\n",
      " - 368s - loss: 0.0651 - acc: 0.9762 - val_loss: 0.3906 - val_acc: 0.8896\n",
      "-----  doing from trining/test: 320000 - 360000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 367s - loss: 0.2566 - acc: 0.8977 - val_loss: 0.2395 - val_acc: 0.9041\n",
      "Epoch 2/4\n",
      " - 354s - loss: 0.1627 - acc: 0.9375 - val_loss: 0.2585 - val_acc: 0.8961\n",
      "Epoch 3/4\n",
      " - 356s - loss: 0.1004 - acc: 0.9625 - val_loss: 0.3153 - val_acc: 0.8955\n",
      "Epoch 4/4\n",
      " - 368s - loss: 0.0680 - acc: 0.9754 - val_loss: 0.4086 - val_acc: 0.8915\n",
      "-----  doing from trining/test: 360000 - 400000\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/4\n",
      " - 415s - loss: 0.2458 - acc: 0.9021 - val_loss: 0.2343 - val_acc: 0.9059\n",
      "Epoch 2/4\n",
      " - 354s - loss: 0.1586 - acc: 0.9406 - val_loss: 0.2564 - val_acc: 0.8991\n",
      "Epoch 3/4\n",
      " - 362s - loss: 0.0946 - acc: 0.9666 - val_loss: 0.3082 - val_acc: 0.8986\n",
      "Epoch 4/4\n",
      " - 357s - loss: 0.0650 - acc: 0.9766 - val_loss: 0.3794 - val_acc: 0.8915\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = math.ceil(test_data_info['total_lines']/\\\n",
    "    (train_data_info['total_lines']/test_data_info['total_lines']))\n",
    "test_size = 40000\n",
    "train_size = 40000\n",
    "n = 10\n",
    "runByBatches(model, \n",
    "            train_data_info, \n",
    "            test_data_info, \n",
    "            alphabet, \n",
    "            max_len_text, \n",
    "            train_size, #test_data_info['total_lines'],\n",
    "            test_size, # test batch size\n",
    "            train_size * n # until n chunks of data (n repetitions)\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=alphabet) # , oov_token= ?  \n",
    "tokenizer.word_index = getAlphabetDict(alphabet)\n",
    "t = getDataAndClasses(tokenizer, test_data_info, alphabet, max_len_text, 10000,0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hist_arr[0].history\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistoryList(h):\n",
    "    acc = h['acc']\n",
    "    val_acc = h['val_acc']\n",
    "    loss = h['loss']\n",
    "    val_loss = h['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    #plt.plot(epochs, acc, 'ro', label='Accuracy')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    #plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hist_arr:\n",
    "    plotHistoryList(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM FIRST TEST\n",
    "\"\"\"\n",
    "By using a sample data of 10000 each time, and 10000 for validation, \n",
    "it established that at epoch 3 it was enough since it started to have overfitting\n",
    "\n",
    "[{'val_loss': [0.6950897953033447,\n",
    "   0.6947383718490601,\n",
    "   0.6934752422332764,\n",
    "   0.6931729074478149,\n",
    "   0.6935873050689697],\n",
    "  'val_acc': [0.4875, 0.4875, 0.4875, 0.4875, 0.4875],\n",
    "  'loss': [0.694396387386322,\n",
    "   0.6932769330978393,\n",
    "   0.6929676490783692,\n",
    "   0.6933105537414551,\n",
    "   0.6934012260437011],\n",
    "  'acc': [0.5027, 0.51, 0.5084, 0.5055, 0.5029]},\n",
    " {'val_loss': [0.693684473323822,\n",
    "   0.69328423204422,\n",
    "   0.6938694007873535,\n",
    "   0.693745785331726,\n",
    "   0.693725270652771],\n",
    "  'val_acc': [0.5125, 0.5125, 0.5125, 0.5125, 0.5125],\n",
    "  'loss': [0.6909599201202392,\n",
    "   0.6909819056510925,\n",
    "   0.690941504573822,\n",
    "   0.6910166868209839,\n",
    "   0.6907023878097535],\n",
    "  'acc': [0.5339, 0.5354, 0.5354, 0.5354, 0.5354]},\n",
    " {'val_loss': [0.6928669214248657,\n",
    "   0.6928351986885071,\n",
    "   0.6929179975509644,\n",
    "   0.6928665622711182,\n",
    "   0.6929092376708984],\n",
    "  'val_acc': [0.5125, 0.5125, 0.5125, 0.5125, 0.5125],\n",
    "  'loss': [0.6928800000190735,\n",
    "   0.6927422769546508,\n",
    "   0.692695544910431,\n",
    "   0.6927323137283325,\n",
    "   0.6927251628875732],\n",
    "  'acc': [0.5171, 0.5172, 0.5171, 0.5171, 0.5157]}]\n",
    "\n",
    "\"\"\"\n",
    "# SECOND\n",
    "\n",
    "\"\"\"\n",
    "[{'val_loss': [0.6933708633422851,\n",
    "   0.6928537891387939,\n",
    "   0.692834661102295,\n",
    "   0.6928840206146241,\n",
    "   0.6929055606842041],\n",
    "  'val_acc': [0.4875, 0.5125, 0.5125, 0.5125, 0.5125],\n",
    "  'loss': [0.6934309574127198,\n",
    "   0.6930742263793945,\n",
    "   0.6930086311340332,\n",
    "   0.6929674413681031,\n",
    "   0.6930006246566772],\n",
    "  'acc': [0.50965, 0.51025, 0.51005, 0.5117, 0.50945]},\n",
    " {'val_loss': [0.6928420051574707,\n",
    "   0.6929439851760865,\n",
    "   0.693077564239502,\n",
    "   0.6930943966865539,\n",
    "   0.6930167970657348],\n",
    "  'val_acc': [0.5125, 0.5125, 0.5125, 0.5125, 0.5125],\n",
    "  'loss': [0.6932257574081421,\n",
    "   0.6932153765678406,\n",
    "   0.6932126870155334,\n",
    "   0.6931769281387329,\n",
    "   0.693256987953186],\n",
    "  'acc': [0.5025, 0.5051, 0.50615, 0.50025, 0.49795]}]\n",
    "\"\"\"\n",
    "\n",
    "# Third 30000 / 15000\n",
    "\"\"\"\n",
    "[{'val_loss': [0.6932847999890646,\n",
    "   0.6933213699658711,\n",
    "   0.6930976839383444,\n",
    "   0.6931018094380696,\n",
    "   0.6930792671521505],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6933522695223491,\n",
    "   0.6929458774248759,\n",
    "   0.6930164001146952,\n",
    "   0.6929120743115743,\n",
    "   0.6929684819221497],\n",
    "  'acc': [0.5136000000158946,\n",
    "   0.5124333333333333,\n",
    "   0.5129666666984558,\n",
    "   0.5137000000317892,\n",
    "   0.5138333333651225]},\n",
    " {'val_loss': [0.6931314949353536,\n",
    "   0.6930487598737081,\n",
    "   0.6931946491559347,\n",
    "   0.693049590587616,\n",
    "   0.6932759721120199],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6929239110628764,\n",
    "   0.6928764338493347,\n",
    "   0.6929650987943013,\n",
    "   0.6928932430585225,\n",
    "   0.6928442873636882],\n",
    "  'acc': [0.5129333333015442,\n",
    "   0.5128,\n",
    "   0.5133666666666666,\n",
    "   0.5129999999841054,\n",
    "   0.5129666666666667]}]\n",
    "\"\"\"\n",
    "# Fourth 100 000 / 15 000\n",
    "\"\"\"\n",
    "[{'val_loss': [0.6935770348866781,\n",
    "   0.693049957370758,\n",
    "   0.6931386842091878,\n",
    "   0.6931923212369283,\n",
    "   0.6930569999694824],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.693238242149353,\n",
    "   0.6929247446441651,\n",
    "   0.692936732673645,\n",
    "   0.6928754871940613,\n",
    "   0.6928619545555115],\n",
    "  'acc': [0.5106, 0.512, 0.51234, 0.51267, 0.51267]}]\n",
    "\"\"\"\n",
    "# Fifth 20 000 / 15 000 (5 times)\n",
    "\"\"\"\n",
    "[{'val_loss': [0.6930875172615051,\n",
    "   0.6930843497912089,\n",
    "   0.6934041076978048,\n",
    "   0.6935051878611247,\n",
    "   0.6931250178019206],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6936674816131592,\n",
    "   0.6930888975143432,\n",
    "   0.6951768466949463,\n",
    "   0.693067194366455,\n",
    "   0.6930339816093445],\n",
    "  'acc': [0.50965, 0.50625, 0.51175, 0.51125, 0.5125]},\n",
    " {'val_loss': [0.6931213233629863,\n",
    "   0.6931330739021301,\n",
    "   0.6930682267824809,\n",
    "   0.693061719640096,\n",
    "   0.6931284649848938],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6932584546089172,\n",
    "   0.6931955748558044,\n",
    "   0.6931753499984741,\n",
    "   0.6931837162017822,\n",
    "   0.6932215364456177],\n",
    "  'acc': [0.5019, 0.50075, 0.50005, 0.50375, 0.5013]},\n",
    " {'val_loss': [0.6931777195930481,\n",
    "   0.6935282015800476,\n",
    "   0.6935493802388509,\n",
    "   0.6932427165031433,\n",
    "   0.6935413696289062],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6924584628105164,\n",
    "   0.6921570532798768,\n",
    "   0.6920773757934571,\n",
    "   0.692116604423523,\n",
    "   0.6921314051628112],\n",
    "  'acc': [0.52205, 0.5237, 0.5237, 0.5237, 0.5237]},\n",
    " {'val_loss': [0.6931134839375814,\n",
    "   0.6930499877929688,\n",
    "   0.6930544673919677,\n",
    "   0.6930453857421875,\n",
    "   0.6930874656677246],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6930728750228882,\n",
    "   0.692966617679596,\n",
    "   0.6930215294837951,\n",
    "   0.6930123390197754,\n",
    "   0.6930175746917725],\n",
    "  'acc': [0.50985, 0.5098, 0.50985, 0.50985, 0.5098]},\n",
    " {'val_loss': [0.693133440430959,\n",
    "   0.6931064677874247,\n",
    "   0.6930852600097657,\n",
    "   0.6930747243245443,\n",
    "   0.6930734438578288],\n",
    "  'val_acc': [0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279,\n",
    "   0.5071333333492279],\n",
    "  'loss': [0.6928748257637024,\n",
    "   0.6929110748291015,\n",
    "   0.6928599676132202,\n",
    "   0.6928945977210998,\n",
    "   0.6928995547294616],\n",
    "  'acc': [0.5126, 0.5126, 0.5126, 0.5126, 0.5126]}]\n",
    "\"\"\"\n",
    "# Sixth 40 000 test 40 000 train 10 repetitions\n",
    "\"\"\"\n",
    "[{'val_loss': [0.49881081466674804,\n",
    "   0.4717710014820099,\n",
    "   0.35536931228637697,\n",
    "   0.3205692740917206],\n",
    "  'val_acc': [0.762225, 0.766625, 0.843275, 0.86315],\n",
    "  'loss': [0.6303098274230957,\n",
    "   0.44791171159744264,\n",
    "   0.3468285531997681,\n",
    "   0.27311122870445254],\n",
    "  'acc': [0.61975, 0.794125, 0.849325, 0.8875]},\n",
    " {'val_loss': [0.3053042756080627,\n",
    "   0.308405194234848,\n",
    "   0.308764573431015,\n",
    "   0.3359229224443436],\n",
    "  'val_acc': [0.868175, 0.87735, 0.876225, 0.872425],\n",
    "  'loss': [0.3139024566650391,\n",
    "   0.24463748073577882,\n",
    "   0.19323326365947724,\n",
    "   0.14400334199666978],\n",
    "  'acc': [0.86715, 0.9004, 0.924075, 0.9444]},\n",
    " {'val_loss': [0.27427815690040586,\n",
    "   0.28455733699798585,\n",
    "   0.3371319869041443,\n",
    "   0.370827214550972],\n",
    "  'val_acc': [0.884575, 0.8868, 0.874225, 0.8835],\n",
    "  'loss': [0.27802091131210327,\n",
    "   0.196414630818367,\n",
    "   0.13643594622612,\n",
    "   0.09739455650448799],\n",
    "  'acc': [0.8852, 0.9234, 0.94905, 0.96285]},\n",
    " {'val_loss': [0.24903032941818237,\n",
    "   0.2800765148639679,\n",
    "   0.36023315348625184,\n",
    "   0.3746987383365631],\n",
    "  'val_acc': [0.89795, 0.89075, 0.87445, 0.8891],\n",
    "  'loss': [0.2717844838142395,\n",
    "   0.18792205538749696,\n",
    "   0.12372702569961548,\n",
    "   0.08530010979473591],\n",
    "  'acc': [0.88835, 0.925925, 0.9533, 0.9688]},\n",
    " {'val_loss': [0.26640101244449615,\n",
    "   0.2713427514791489,\n",
    "   0.3376887015104294,\n",
    "   0.3635027582406998],\n",
    "  'val_acc': [0.894275, 0.8952, 0.88575, 0.88995],\n",
    "  'loss': [0.26361796135902404,\n",
    "   0.17128136606216432,\n",
    "   0.10989078753292561,\n",
    "   0.07645529963374138],\n",
    "  'acc': [0.89225, 0.934725, 0.9599, 0.9723]},\n",
    " {'val_loss': [0.24720766174793243,\n",
    "   0.27378872973918916,\n",
    "   0.28408009884357455,\n",
    "   0.34479169399738313],\n",
    "  'val_acc': [0.899, 0.899175, 0.8898, 0.890075],\n",
    "  'loss': [0.2508507478237152,\n",
    "   0.16069013158082962,\n",
    "   0.10054244674444199,\n",
    "   0.07001338513493538],\n",
    "  'acc': [0.8972, 0.9392, 0.96255, 0.97475]},\n",
    " {'val_loss': [0.2465739718914032,\n",
    "   0.27122063734531404,\n",
    "   0.31714685039520263,\n",
    "   0.3997681526184082],\n",
    "  'val_acc': [0.89915, 0.901175, 0.8907, 0.89325],\n",
    "  'loss': [0.2506037933111191,\n",
    "   0.16453868765830992,\n",
    "   0.09823134387135506,\n",
    "   0.0721335696876049],\n",
    "  'acc': [0.901025, 0.9374, 0.964775, 0.973725]},\n",
    " {'val_loss': [0.23710816338062288,\n",
    "   0.2629594801664352,\n",
    "   0.29442494423389437,\n",
    "   0.39059445977211],\n",
    "  'val_acc': [0.9056, 0.898675, 0.89235, 0.889625],\n",
    "  'loss': [0.2562410287857056,\n",
    "   0.16355337309837342,\n",
    "   0.09930451941490173,\n",
    "   0.06507483651936054],\n",
    "  'acc': [0.897625, 0.939275, 0.964, 0.9762]},\n",
    " {'val_loss': [0.23953482286930083,\n",
    "   0.2585186312675476,\n",
    "   0.3152854981899261,\n",
    "   0.4086348847866058],\n",
    "  'val_acc': [0.904125, 0.896125, 0.895525, 0.8915],\n",
    "  'loss': [0.25655409202575685,\n",
    "   0.1626516498208046,\n",
    "   0.10038786889314652,\n",
    "   0.06800436522364617],\n",
    "  'acc': [0.897675, 0.9375, 0.9625, 0.9754]},\n",
    " {'val_loss': [0.2342738193511963,\n",
    "   0.2563602669239044,\n",
    "   0.3081534208774567,\n",
    "   0.3793575043439865],\n",
    "  'val_acc': [0.9059, 0.8991, 0.898575, 0.891475],\n",
    "  'loss': [0.24582136721611023,\n",
    "   0.15862737138271332,\n",
    "   0.09455530262589455,\n",
    "   0.06496932366937398],\n",
    "  'acc': [0.902075, 0.940575, 0.96665, 0.9766]}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': [0.49881081466674804,\n",
       "   0.4717710014820099,\n",
       "   0.35536931228637697,\n",
       "   0.3205692740917206],\n",
       "  'val_acc': [0.762225, 0.766625, 0.843275, 0.86315],\n",
       "  'loss': [0.6303098274230957,\n",
       "   0.44791171159744264,\n",
       "   0.3468285531997681,\n",
       "   0.27311122870445254],\n",
       "  'acc': [0.61975, 0.794125, 0.849325, 0.8875]},\n",
       " {'val_loss': [0.3053042756080627,\n",
       "   0.308405194234848,\n",
       "   0.308764573431015,\n",
       "   0.3359229224443436],\n",
       "  'val_acc': [0.868175, 0.87735, 0.876225, 0.872425],\n",
       "  'loss': [0.3139024566650391,\n",
       "   0.24463748073577882,\n",
       "   0.19323326365947724,\n",
       "   0.14400334199666978],\n",
       "  'acc': [0.86715, 0.9004, 0.924075, 0.9444]},\n",
       " {'val_loss': [0.27427815690040586,\n",
       "   0.28455733699798585,\n",
       "   0.3371319869041443,\n",
       "   0.370827214550972],\n",
       "  'val_acc': [0.884575, 0.8868, 0.874225, 0.8835],\n",
       "  'loss': [0.27802091131210327,\n",
       "   0.196414630818367,\n",
       "   0.13643594622612,\n",
       "   0.09739455650448799],\n",
       "  'acc': [0.8852, 0.9234, 0.94905, 0.96285]},\n",
       " {'val_loss': [0.24903032941818237,\n",
       "   0.2800765148639679,\n",
       "   0.36023315348625184,\n",
       "   0.3746987383365631],\n",
       "  'val_acc': [0.89795, 0.89075, 0.87445, 0.8891],\n",
       "  'loss': [0.2717844838142395,\n",
       "   0.18792205538749696,\n",
       "   0.12372702569961548,\n",
       "   0.08530010979473591],\n",
       "  'acc': [0.88835, 0.925925, 0.9533, 0.9688]},\n",
       " {'val_loss': [0.26640101244449615,\n",
       "   0.2713427514791489,\n",
       "   0.3376887015104294,\n",
       "   0.3635027582406998],\n",
       "  'val_acc': [0.894275, 0.8952, 0.88575, 0.88995],\n",
       "  'loss': [0.26361796135902404,\n",
       "   0.17128136606216432,\n",
       "   0.10989078753292561,\n",
       "   0.07645529963374138],\n",
       "  'acc': [0.89225, 0.934725, 0.9599, 0.9723]},\n",
       " {'val_loss': [0.24720766174793243,\n",
       "   0.27378872973918916,\n",
       "   0.28408009884357455,\n",
       "   0.34479169399738313],\n",
       "  'val_acc': [0.899, 0.899175, 0.8898, 0.890075],\n",
       "  'loss': [0.2508507478237152,\n",
       "   0.16069013158082962,\n",
       "   0.10054244674444199,\n",
       "   0.07001338513493538],\n",
       "  'acc': [0.8972, 0.9392, 0.96255, 0.97475]},\n",
       " {'val_loss': [0.2465739718914032,\n",
       "   0.27122063734531404,\n",
       "   0.31714685039520263,\n",
       "   0.3997681526184082],\n",
       "  'val_acc': [0.89915, 0.901175, 0.8907, 0.89325],\n",
       "  'loss': [0.2506037933111191,\n",
       "   0.16453868765830992,\n",
       "   0.09823134387135506,\n",
       "   0.0721335696876049],\n",
       "  'acc': [0.901025, 0.9374, 0.964775, 0.973725]},\n",
       " {'val_loss': [0.23710816338062288,\n",
       "   0.2629594801664352,\n",
       "   0.29442494423389437,\n",
       "   0.39059445977211],\n",
       "  'val_acc': [0.9056, 0.898675, 0.89235, 0.889625],\n",
       "  'loss': [0.2562410287857056,\n",
       "   0.16355337309837342,\n",
       "   0.09930451941490173,\n",
       "   0.06507483651936054],\n",
       "  'acc': [0.897625, 0.939275, 0.964, 0.9762]},\n",
       " {'val_loss': [0.23953482286930083,\n",
       "   0.2585186312675476,\n",
       "   0.3152854981899261,\n",
       "   0.4086348847866058],\n",
       "  'val_acc': [0.904125, 0.896125, 0.895525, 0.8915],\n",
       "  'loss': [0.25655409202575685,\n",
       "   0.1626516498208046,\n",
       "   0.10038786889314652,\n",
       "   0.06800436522364617],\n",
       "  'acc': [0.897675, 0.9375, 0.9625, 0.9754]},\n",
       " {'val_loss': [0.2342738193511963,\n",
       "   0.2563602669239044,\n",
       "   0.3081534208774567,\n",
       "   0.3793575043439865],\n",
       "  'val_acc': [0.9059, 0.8991, 0.898575, 0.891475],\n",
       "  'loss': [0.24582136721611023,\n",
       "   0.15862737138271332,\n",
       "   0.09455530262589455,\n",
       "   0.06496932366937398],\n",
       "  'acc': [0.902075, 0.940575, 0.96665, 0.9766]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
