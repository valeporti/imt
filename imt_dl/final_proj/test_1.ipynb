{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import initializers , preprocessing, utils\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}\n",
      "\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Alphabet\n",
    "# \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "numbers = \"0123456789\"\n",
    "other_char = \",;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\" # original: -,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}, changed: -,;.!?:’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\n",
    "new_line = \"\\n\"\n",
    "\n",
    "alphabet = letters + numbers + other_char + new_line\n",
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text\n",
    "\n",
    "text = \"hel o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Quantization\n",
    "\n",
    "# [70 rows (the alphabet), text lenght columns]\n",
    "\n",
    "def quantize_text(alphabet, text):\n",
    "    results = np.zeros((len(alphabet), len(text)))\n",
    "    for i, char in enumerate(text):\n",
    "        if char.lower() in alphabet:\n",
    "            results[alphabet.index(char.lower()), i] = 1\n",
    "    return results\n",
    "\n",
    "quantized_text = quantize_text(alphabet, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(quantized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-38e632fdaf27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtest_data_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetListOfLabelsAndLinesFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "# type of text\n",
    "# __label__2 This is my first PDA/Organizer: I purchased this about 4 months ago and it really is easy to use especially if you are familiar with Microsoft Word and Excel. I use it to copy files from my PC that I want to have handy. The only thing I wish it had is a backlight. But that hasn't come into play too often. I usually have to charge it every 2 or 3 days. I recommend this product if you are not worried about having a lot accessories to go with it, because they are not that many available.\n",
    "# __label__1 Piece of Crap: I have ordered thousands of items in my lifetime, and bar none, this is the biggest piece of crap I have ever received. It is supposed to come completely put together, but when it arrived, there were no less than 10 little parts that had come off the screen assembly and one screen that was loose and all of its parts dangling. The first thing that happenned is that I cut myself on one of the screens because the protective side had come off. I was bleeding for quite some time. The second thing I noticed was the extremely poor quality of the material and assembly. I am very handy by nature, but even I had to give up trying to put this piece of junk back together again. Plus it was not worth it, even if I had put it together again, all I would have then had was a piece of crap fireplace screen.\n",
    "# data from  https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "\n",
    "dir_name = './data/'\n",
    "test_file_name = 'test.ft.txt'\n",
    "train_file_name = 'train.ft.txt'\n",
    "\n",
    "\n",
    "def getListOfLabelsAndLinesFromFile(file_name): # label 1 = 0, label 2 = 1\n",
    "    \n",
    "    labels = []\n",
    "    lines = []\n",
    "    total_labels = []\n",
    "    count = 0\n",
    "    file = open(file_name, 'r')\n",
    "    for line in file:\n",
    "        match = re.match('(__label__([0-9])) (.*)', line)\n",
    "        if match:\n",
    "            label = int(match.group(2)) - 1 # to make the label start in 0 (ZERO)\n",
    "            text = match.group(3)\n",
    "            if not label in total_labels: total_labels.append(label)\n",
    "            labels.append(label)\n",
    "            lines.append(text)\n",
    "        else:\n",
    "            print('---- ERROR ----')\n",
    "        count += 1\n",
    "    file.close()\n",
    "    return { 'labels': labels, 'lines': lines, 'total_labels': total_labels, 'total_lines': count }\n",
    "\n",
    "train_data_info = getListOfLabelsAndLinesFromFile(dir_name + train_file_name)\n",
    "test_data_info = getListOfLabelsAndLinesFromFile(dir_name + test_file_name)\n",
    "\n",
    "print(test_data_info['total_lines']))\n",
    "print(train_data_info['total_lines']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Embedding\n",
    "max_len_text = 1014\n",
    "max_features_for_embedding = len(alphabet)\n",
    "output_from_embedding = 128\n",
    "\n",
    "# Large Model\n",
    "initializer_large = { 'mean': 0.0, 'stddev': 0.02}\n",
    "activation_conv = 'relu'\n",
    "conv_output_dim = 256\n",
    "pool_size = 3\n",
    "kernel_size_7 = 7 # first_WITH_maxpool\n",
    "kernel_size_3 = 3 # intermediate_NO_maxpool\n",
    "\n",
    "# Small model\n",
    "initializer_small = { 'mean': 0.0, 'stddev': 0.05}\n",
    "activation_dens = 'relu'\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1014, 128)         8832      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1008, 256)         229632    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 336, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 330, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 106, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 104, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 102, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 11,449,473\n",
      "Trainable params: 11,449,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "# https://medium.com/@romannempyre/sentiment-analysis-using-1d-convolutional-neural-networks-part-1-f8b6316489a2\n",
    "# https://github.com/chaitjo/character-level-cnn\n",
    "# https://missinglink.ai/guides/deep-learning-frameworks/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
    "# https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=5c4a324fc922\n",
    "# https://medium.com/@bramblexu/character-level-cnn-with-keras-50391c3adf33\n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "#model.add(layers.Input(shape=(max_len_text,),  dtype='int32'))\n",
    "\n",
    "# AS the one hot approach, leave us with a very sparse and high dimensional matrix, we apply embedding\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "# Embedding ( \n",
    "#    max_features/input_dim = we got just 70 different types of inputs / Size of the vocabulary, \n",
    "#    output_dim = learn N dimensional embeddings for each of the input_dim words/characters,\n",
    "#    input_length needed to flatten = cut the words/characters of each element to that lenght so the max\n",
    "#                                     qty of elements in each phrase will be that one\n",
    "# )    \n",
    "model.add(layers.Embedding(max_features_for_embedding, output_from_embedding, input_length=max_len_text))\n",
    "# After the Embedding layer,\n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "# Output: (batch dimension, input_length, output_dim)\n",
    "\n",
    "# Convolutional Layer 1D\n",
    "# In text \n",
    "# Conv1D(\n",
    "#    filters = dimensionality of the output space\n",
    "#    kernel_size = window_size\n",
    "#    stride = position jumps of the window (defaults to 1)\n",
    "# )\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv, kernel_initializer=initializer_large_model))\n",
    "\n",
    "# Max Pooling 1D\n",
    "# MaxPool1D(\n",
    "#    pool_size = window size\n",
    "#    strides = by default equals pool_size, so each window doesn't overlap\n",
    "# )\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "\n",
    "# Second part, full connected layers\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation=activation_dens, kernel_initializer=initializer_large_model))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(1024, activation=activation_dens))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "# Last layer, according to problem to solve\n",
    "# (just have 2 classes)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer and loss function and metrics to return \n",
    "# binary_crossentropy ? \n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7f36a8c1a780>\n"
     ]
    }
   ],
   "source": [
    "# treat the data before passing it to the model\n",
    "# train_data_info \n",
    "# test_data_info \n",
    "# { 'labels': , 'lines': , 'total_labels': , 'total_lines':  }\n",
    "\n",
    "small_file_name = 'small.txt'\n",
    "small_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=alphabet) # , oov_token= ?  \n",
    "tokenizer.fit_on_texts(small_data_info['lines'])\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#one_hot_results = tokenizer.texts_to_matrix(train_data_info['lines'], mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1, 'e': 2, 't': 3, 'i': 4, 'a': 5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsmall_file_name = 'small.txt'\\nsmall_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\\n\\ndef getAllLinesQuantized(lines, alphabet):\\n    quantized_text = []\\n    for line in lines:\\n        quantized = quantize_text(alphabet, line)\\n        quantized_text.append(quantized)\\n    return quantized_text\\n\\nquantized_text = getAllLinesQuantized(small_data_info['lines'], final_string)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize my way\n",
    "# AVOID.. A LOT OF MEMORY!!!\n",
    "\"\"\"\n",
    "small_file_name = 'small.txt'\n",
    "small_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\n",
    "\n",
    "def getAllLinesQuantized(lines, alphabet):\n",
    "    quantized_text = []\n",
    "    for line in lines:\n",
    "        quantized = quantize_text(alphabet, line)\n",
    "        quantized_text.append(quantized)\n",
    "    return quantized_text\n",
    "\n",
    "quantized_text = getAllLinesQuantized(small_data_info['lines'], final_string)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 9, 2, 5, 3, 1, 13, 12, 28, 1, 14, 17, 1, 11, 6, 22, 2, 11, 17, 1, 24, 5, 3, 1, 10, 5, 7, 1, 6, 8, 2, 1, 6, 18, 1, 3, 10, 2, 1, 15, 9, 2, 5, 3, 1, 22, 6, 4, 13, 2, 7, 1, 6, 18, 1, 10, 2, 9, 1, 15, 2, 8, 2, 9, 5, 3, 4, 6, 8, 19, 1, 4, 1, 10, 5, 22, 2, 1, 11, 4, 7, 3, 2, 8, 2, 12, 1, 3, 6, 1, 3, 10, 4, 7, 1, 13, 12, 1, 18, 6, 9, 1, 17, 2, 5, 9, 7, 1, 5, 8, 12, 1, 4, 1, 7, 3, 4, 11, 11, 1, 11, 6, 22, 2, 1, 4, 3, 19, 1, 20, 10, 2, 8, 1, 4, 27, 14, 1, 4, 8, 1, 5, 1, 15, 6, 6, 12, 1, 14, 6, 6, 12, 1, 4, 3, 1, 14, 5, 23, 2, 7, 1, 14, 2, 1, 18, 2, 2, 11, 1, 21, 2, 3, 3, 2, 9, 19, 1, 5, 1, 21, 5, 12, 1, 14, 6, 6, 12, 1, 26, 16, 7, 3, 1, 2, 22, 5, 24, 6, 9, 5, 3, 2, 7, 1, 11, 4, 23, 2, 1, 7, 16, 15, 5, 9, 1, 4, 8, 1, 3, 10, 2, 1, 9, 5, 4, 8, 19, 1, 3, 10, 4, 7, 1, 13, 12, 1, 26, 16, 7, 3, 1, 6, 6, 34, 2, 7, 1, 11, 4, 18, 2, 19, 1, 22, 6, 13, 5, 11, 7, 1, 5, 9, 2, 1, 26, 16, 7, 5, 3, 1, 7, 3, 16, 16, 8, 8, 4, 8, 15, 1, 5, 8, 12, 1, 11, 17, 9, 4, 13, 7, 1, 26, 16, 7, 3, 1, 23, 4, 11, 11, 19, 1, 6, 8, 2, 1, 6, 18, 1, 11, 4, 18, 2, 27, 7, 1, 10, 4, 12, 12, 2, 8, 1, 15, 2, 14, 7, 19, 1, 3, 10, 4, 7, 1, 4, 7, 1, 5, 1, 12, 2, 7, 2, 9, 3, 1, 4, 7, 11, 2, 1, 13, 12, 1, 4, 8, 1, 14, 17, 1, 21, 6, 6, 23, 19, 1, 20, 10, 17, 1, 7, 10, 2, 1, 8, 2, 22, 2, 9, 1, 14, 5, 12, 2, 1, 4, 3, 1, 21, 4, 15, 1, 4, 7, 1, 26, 16, 7, 3, 1, 21, 2, 17, 6, 8, 12, 1, 14, 2, 19, 1, 2, 22, 2, 9, 17, 3, 4, 14, 2, 1, 4, 1, 24, 11, 5, 17, 1, 3, 10, 4, 7, 25, 1, 8, 6, 1, 14, 5, 3, 3, 2, 9, 1, 21, 11, 5, 13, 23, 25, 1, 20, 10, 4, 3, 2, 25, 1, 17, 6, 16, 8, 15, 25, 1, 6, 11, 12, 25, 1, 14, 5, 11, 2, 25, 1, 18, 2, 14, 5, 11, 2, 1, 2, 22, 2, 9, 17, 21, 6, 12, 17, 1, 7, 5, 17, 7, 1, 6, 8, 2, 1, 3, 10, 4, 8, 15, 1, 29, 20, 10, 6, 1, 20, 5, 7, 1, 3, 10, 5, 3, 1, 7, 4, 8, 15, 4, 8, 15, 1, 35, 29], [6, 8, 2, 1, 6, 18, 1, 3, 10, 2, 1, 21, 2, 7, 3, 1, 15, 5, 14, 2, 1, 14, 16, 7, 4, 13, 1, 7, 6, 16, 8, 12, 3, 9, 5, 13, 23, 7, 1, 30, 1, 18, 6, 9, 1, 5, 1, 15, 5, 14, 2, 1, 4, 1, 12, 4, 12, 8, 27, 3, 1, 9, 2, 5, 11, 11, 17, 1, 24, 11, 5, 17, 28, 1, 12, 2, 7, 24, 4, 3, 2, 1, 3, 10, 2, 1, 18, 5, 13, 3, 1, 3, 10, 5, 3, 1, 4, 1, 10, 5, 22, 2, 1, 6, 8, 11, 17, 1, 24, 11, 5, 17, 2, 12, 1, 5, 1, 7, 14, 5, 11, 11, 1, 24, 6, 9, 3, 4, 6, 8, 1, 6, 18, 1, 3, 10, 2, 1, 15, 5, 14, 2, 25, 1, 3, 10, 2, 1, 14, 16, 7, 4, 13, 1, 4, 1, 10, 2, 5, 9, 12, 1, 31, 24, 11, 16, 7, 1, 3, 10, 2, 1, 13, 6, 8, 8, 2, 13, 3, 4, 6, 8, 1, 3, 6, 1, 13, 10, 9, 6, 8, 6, 1, 3, 9, 4, 15, 15, 2, 9, 1, 20, 10, 4, 13, 10, 1, 20, 5, 7, 1, 15, 9, 2, 5, 3, 1, 5, 7, 1, 20, 2, 11, 11, 32, 1, 11, 2, 12, 1, 14, 2, 1, 3, 6, 1, 24, 16, 9, 13, 10, 5, 7, 2, 1, 3, 10, 2, 1, 7, 6, 16, 8, 12, 3, 9, 5, 13, 23, 25, 1, 5, 8, 12, 1, 4, 3, 1, 9, 2, 14, 5, 4, 8, 7, 1, 6, 8, 2, 1, 6, 18, 1, 14, 17, 1, 18, 5, 22, 6, 9, 4, 3, 2, 1, 5, 11, 21, 16, 14, 7, 19, 1, 3, 10, 2, 9, 2, 1, 4, 7, 1, 5, 8, 1, 4, 8, 13, 9, 2, 12, 4, 21, 11, 2, 1, 14, 4, 36, 1, 6, 18, 1, 18, 16, 8, 25, 1, 2, 24, 4, 13, 25, 1, 5, 8, 12, 1, 2, 14, 6, 3, 4, 6, 8, 5, 11, 1, 7, 6, 8, 15, 7, 19, 1, 3, 10, 6, 7, 2, 1, 7, 5, 12, 1, 5, 8, 12, 1, 21, 2, 5, 16, 3, 4, 18, 16, 11, 1, 3, 9, 5, 13, 23, 7, 1, 4, 1, 2, 7, 24, 2, 13, 4, 5, 11, 11, 17, 1, 11, 4, 23, 2, 25, 1, 5, 7, 1, 3, 10, 2, 9, 2, 27, 7, 1, 8, 6, 3, 1, 3, 6, 6, 1, 14, 5, 8, 17, 1, 6, 18, 1, 3, 10, 6, 7, 2, 1, 23, 4, 8, 12, 7, 1, 6, 18, 1, 7, 6, 8, 15, 7, 1, 4, 8, 1, 14, 17, 1, 6, 3, 10, 2, 9, 1, 22, 4, 12, 2, 6, 1, 15, 5, 14, 2, 1, 7, 6, 16, 8, 12, 3, 9, 5, 13, 23, 7, 19, 1, 4, 1, 14, 16, 7, 3, 1, 5, 12, 14, 4, 3, 1, 3, 10, 5, 3, 1, 6, 8, 2, 1, 6, 18, 1, 3, 10, 2, 1, 7, 6, 8, 15, 7, 1, 31, 11, 4, 18, 2, 30, 5, 1, 12, 4, 7, 3, 5, 8, 3, 1, 24, 9, 6, 14, 4, 7, 2, 32, 1, 10, 5, 7, 1, 21, 9, 6, 16, 15, 10, 3, 1, 3, 2, 5, 9, 7, 1, 3, 6, 1, 14, 17, 1, 2, 17, 2, 7, 1, 6, 8, 1, 14, 5, 8, 17, 1, 6, 13, 13, 5, 7, 4, 6, 8, 7, 19, 14, 17, 1, 6, 8, 2, 1, 13, 6, 14, 24, 11, 5, 4, 8, 3, 1, 5, 21, 6, 16, 3, 1, 3, 10, 4, 7, 1, 7, 6, 16, 8, 12, 3, 9, 5, 13, 23, 1, 4, 7, 1, 3, 10, 5, 3, 1, 3, 10, 2, 17, 1, 16, 7, 2, 1, 15, 16, 4, 3, 5, 9, 1, 18, 9, 2, 3, 3, 4, 8, 15, 1, 2, 18, 18, 2, 13, 3, 7, 1, 4, 8, 1, 14, 5, 8, 17, 1, 6, 18, 1, 3, 10, 2, 1, 7, 6, 8, 15, 7, 25, 1, 20, 10, 4, 13, 10, 1, 4, 1, 18, 4, 8, 12, 1, 12, 4, 7, 3, 9, 5, 13, 3, 4, 8, 15, 19, 1, 21, 16, 3, 1, 2, 22, 2, 8, 1, 4, 18, 1, 3, 10, 6, 7, 2, 1, 20, 2, 9, 2, 8, 27, 3, 1, 4, 8, 13, 11, 16, 12, 2, 12, 1, 4, 1, 20, 6, 16, 11, 12, 1, 7, 3, 4, 11, 11, 1, 13, 6, 8, 7, 4, 12, 2, 9, 1, 3, 10, 2, 1, 13, 6, 11, 11, 2, 13, 3, 4, 6, 8, 1, 20, 6, 9, 3, 10, 1, 4, 3, 19], [21, 5, 3, 3, 2, 9, 4, 2, 7, 1, 12, 4, 2, 12, 1, 20, 4, 3, 10, 4, 8, 1, 5, 1, 17, 2, 5, 9, 1, 19, 19, 19, 28, 1, 4, 1, 21, 6, 16, 15, 10, 3, 1, 3, 10, 4, 7, 1, 13, 10, 5, 9, 15, 2, 9, 1, 4, 8, 1, 26, 16, 11, 1, 37, 33, 33, 38, 1, 5, 8, 12, 1, 4, 3, 1, 20, 6, 9, 23, 2, 12, 1, 6, 23, 1, 18, 6, 9, 1, 5, 1, 20, 10, 4, 11, 2, 19, 1, 3, 10, 2, 1, 12, 2, 7, 4, 15, 8, 1, 4, 7, 1, 8, 4, 13, 2, 1, 5, 8, 12, 1, 13, 6, 8, 22, 2, 8, 4, 2, 8, 3, 19, 1, 10, 6, 20, 2, 22, 2, 9, 25, 1, 5, 18, 3, 2, 9, 1, 5, 21, 6, 16, 3, 1, 5, 1, 17, 2, 5, 9, 25, 1, 3, 10, 2, 1, 21, 5, 3, 3, 2, 9, 4, 2, 7, 1, 20, 6, 16, 11, 12, 1, 8, 6, 3, 1, 10, 6, 11, 12, 1, 5, 1, 13, 10, 5, 9, 15, 2, 19, 1, 14, 4, 15, 10, 3, 1, 5, 7, 1, 20, 2, 11, 11, 1, 26, 16, 7, 3, 1, 15, 2, 3, 1, 5, 11, 23, 5, 11, 4, 8, 2, 1, 12, 4, 7, 24, 6, 7, 5, 21, 11, 2, 7, 25, 1, 6, 9, 1, 11, 6, 6, 23, 1, 2, 11, 7, 2, 20, 10, 2, 9, 2, 1, 18, 6, 9, 1, 5, 1, 13, 10, 5, 9, 15, 2, 9, 1, 3, 10, 5, 3, 1, 13, 6, 14, 2, 7, 1, 20, 4, 3, 10, 1, 21, 5, 3, 3, 2, 9, 4, 2, 7, 1, 3, 10, 5, 3, 1, 10, 5, 22, 2, 1, 21, 2, 3, 3, 2, 9, 1, 7, 3, 5, 17, 4, 8, 15, 1, 24, 6, 20, 2, 9, 19]]\n"
     ]
    }
   ],
   "source": [
    "# Convert string to index\n",
    "sequences = tokenizer.texts_to_sequences(small_data_info['lines'])\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1014)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because text have different length, \n",
    "# we have to make all text as the same length, so the CNN can handle the batch data.\n",
    "# so that max we got 1014 and if the text is inferior, it will be filled with zeroes\n",
    "# we use \"padding post so that the zeroes goes to the end, and not at the beggining\"\n",
    "data = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len_text, padding='post')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  9,  2, ...,  0,  0,  0],\n",
       "       [ 6,  8,  2, ...,  0,  0,  0],\n",
       "       [21,  5,  3, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_info['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_info['total_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = utils.to_categorical(small_data_info['labels'], num_classes=len(small_data_info['total_labels']))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train and Test Data Sets\n",
    "\n",
    "# Convert string to index\n",
    "sequences_train = tokenizer.texts_to_sequences(train_data_info['lines'])\n",
    "#sequences_test = tokenizer.texts_to_sequences(test_data_info['lines'])\n",
    "# Padding\n",
    "data_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=max_len_text, padding='post')\n",
    "#data_test = preprocessing.sequence.pad_sequences(sequences_test, maxlen=max_len_text, padding='post')\n",
    "#print(data_train.shape)\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes indexation\n",
    "train_classes = utils.to_categorical(train_data_info['labels'], num_classes=len(train_data_info['total_labels']))\n",
    "test_classes = utils.to_categorical(test_data_info['labels'], num_classes=len(test_data_info['total_labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "history = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ',': 37,\n",
       " ';': 38,\n",
       " '.': 39,\n",
       " '!': 40,\n",
       " '?': 41,\n",
       " ':': 42,\n",
       " \"'\": 43,\n",
       " '\"': 44,\n",
       " '/': 45,\n",
       " '\\\\': 46,\n",
       " '|': 47,\n",
       " '_': 48,\n",
       " '@': 49,\n",
       " '#': 50,\n",
       " '$': 51,\n",
       " '%': 52,\n",
       " '^': 53,\n",
       " '&': 54,\n",
       " '*': 55,\n",
       " '~': 56,\n",
       " '`': 57,\n",
       " '+': 58,\n",
       " '-': 59,\n",
       " '=': 60,\n",
       " '<': 61,\n",
       " '>': 62,\n",
       " '(': 63,\n",
       " ')': 64,\n",
       " '[': 65,\n",
       " ']': 66,\n",
       " '{': 67,\n",
       " '}': 68,\n",
       " '\\n': 69}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAlphabetDict(alpha):\n",
    "    d = {}; c = 1\n",
    "    for a in alpha: d[a] = c; c += 1\n",
    "    return d\n",
    "\n",
    "getAlphabetDict(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndClasses(tokenizer, info, alpha, max_len_text, batch_len = None, from_i = 0):\n",
    "        \n",
    "    lines = info['lines']\n",
    "    labels = info['labels']\n",
    "    if batch_len != None: \n",
    "        lines = lines[from_i:(from_i + batch_len)]\n",
    "        labels = labels[from_i:(from_i + batch_len)]\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    \"\"\"EX: [[15, 9, 2, 5, 3, 1, 13, 12, 28, 1, 14, 17 ...\"\"\" # Each, number represents a letter in the alphabet\n",
    "    data = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len_text, padding='post')\n",
    "    # Data of shape (quantity of info (sentences), max_len_text)\n",
    "    \"\"\"EX: array([[15,  9,  2, ...,  0,  0,  0],\n",
    "       [ 6,  8,  2, ...,  0,  0,  0],\n",
    "       [21,  5,  3, ...,  0,  0,  0]], dtype=int32)\"\"\"  \n",
    "    \n",
    "    classes = utils.to_categorical(labels, num_classes=len(info['total_labels']))\n",
    "    \"\"\"EX: array([[0., 1.],\n",
    "       [0., 1.],\n",
    "       [1., 0.]], dtype=float32)\"\"\"\n",
    "    \n",
    "    return {'data': data, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runByBatches(model, train_info, test_info, alpha, max_len_text, batch_size):\n",
    "    \n",
    "    # Preprocess Data \n",
    "    tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=alpha) # , oov_token= ?  \n",
    "    tokenizer.word_index = getAlphabetDict(alpha)\n",
    "    \"\"\"EX: {'a': 1,\n",
    "         'b': 2,\n",
    "         'c': 3 ... \"\"\"\n",
    "    \n",
    "    test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text)\n",
    "    \n",
    "    total_train_len = train_info['total_lines']\n",
    "    count = 0\n",
    "    hist_arr = []\n",
    "    while count < total_train_len:\n",
    "        # Get elements\n",
    "        size = batch_size if batch_size < (total_train_len - count) else total_train_len - count\n",
    "        train_data = getDataAndClasses(tokenizer, train_info, alpha, max_len_text, size, count)\n",
    "        print(f'doing from: {count} - {size}')\n",
    "        count += batch_size   \n",
    "        history = model.fit(train_data['data'], \n",
    "                            train_data['classes'],\n",
    "                            validation_data=(test_data['data'], test_data['classes']),\n",
    "                            epochs=5, # 5000\n",
    "                            batch_size=128\n",
    "                           )\n",
    "        hist_arr.append(history)\n",
    "        train_data = None\n",
    "    return hist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing from: 0 - 400000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (1,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8edc1a6b33f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mmax_len_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         test_data_info['total_lines'])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0e0ac08bdf67>\u001b[0m in \u001b[0;36mrunByBatches\u001b[0;34m(model, train_info, test_info, alpha, max_len_text, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                            )\n\u001b[1;32m     27\u001b[0m         \u001b[0mhist_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (1,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "hist_arr = runByBatches(model, \n",
    "                        train_data_info, \n",
    "                        test_data_info, \n",
    "                        alphabet, \n",
    "                        max_len_text, \n",
    "                        test_data_info['total_lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
