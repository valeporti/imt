{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import initializers , preprocessing, utils\n",
    "import numpy as np\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}\n",
      "\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Alphabet\n",
    "# \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "numbers = \"0123456789\"\n",
    "other_char = \",;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\" # original: -,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}, changed: -,;.!?:’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\n",
    "new_line = \"\\n\"\n",
    "\n",
    "alphabet = letters + numbers + other_char + new_line\n",
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "3600000\n"
     ]
    }
   ],
   "source": [
    "# type of text\n",
    "# __label__2 This is my first PDA/Organizer: I purchased this about 4 months ago and it really is easy to use especially if you are familiar with Microsoft Word and Excel. I use it to copy files from my PC that I want to have handy. The only thing I wish it had is a backlight. But that hasn't come into play too often. I usually have to charge it every 2 or 3 days. I recommend this product if you are not worried about having a lot accessories to go with it, because they are not that many available.\n",
    "# __label__1 Piece of Crap: I have ordered thousands of items in my lifetime, and bar none, this is the biggest piece of crap I have ever received. It is supposed to come completely put together, but when it arrived, there were no less than 10 little parts that had come off the screen assembly and one screen that was loose and all of its parts dangling. The first thing that happenned is that I cut myself on one of the screens because the protective side had come off. I was bleeding for quite some time. The second thing I noticed was the extremely poor quality of the material and assembly. I am very handy by nature, but even I had to give up trying to put this piece of junk back together again. Plus it was not worth it, even if I had put it together again, all I would have then had was a piece of crap fireplace screen.\n",
    "# data from  https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "\n",
    "dir_name = './data/'\n",
    "test_file_name = 'test.ft.txt'\n",
    "train_file_name = 'train.ft.txt'\n",
    "\n",
    "\n",
    "def getListOfLabelsAndLinesFromFile(file_name): # label 1 = 0, label 2 = 1\n",
    "    \n",
    "    labels = []\n",
    "    lines = []\n",
    "    total_labels = []\n",
    "    count = 0\n",
    "    file = open(file_name, 'r')\n",
    "    for line in file:\n",
    "        match = re.match('(__label__([0-9])) (.*)', line)\n",
    "        if match:\n",
    "            label = int(match.group(2)) - 1 # to make the label start in 0 (ZERO)\n",
    "            text = match.group(3)\n",
    "            if not label in total_labels: total_labels.append(label)\n",
    "            labels.append(label)\n",
    "            lines.append(text)\n",
    "        else:\n",
    "            print('---- ERROR ----')\n",
    "        count += 1\n",
    "    file.close()\n",
    "    return { 'labels': labels, 'lines': lines, 'total_labels': total_labels, 'total_lines': count }\n",
    "\n",
    "train_data_info = getListOfLabelsAndLinesFromFile(dir_name + train_file_name)\n",
    "test_data_info = getListOfLabelsAndLinesFromFile(dir_name + test_file_name)\n",
    "\n",
    "print(test_data_info['total_lines'])\n",
    "print(train_data_info['total_lines'])\n",
    "# ratio 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Embedding\n",
    "max_len_text = 1014\n",
    "max_features_for_embedding = len(alphabet)\n",
    "output_from_embedding = 128\n",
    "\n",
    "# Large Model\n",
    "initializer_large = { 'mean': 0.0, 'stddev': 0.02}\n",
    "activation_conv = 'relu'\n",
    "conv_output_dim = 256\n",
    "pool_size = 3\n",
    "kernel_size_7 = 7 # first_WITH_maxpool\n",
    "kernel_size_3 = 3 # intermediate_NO_maxpool\n",
    "\n",
    "# Small model\n",
    "initializer_small = { 'mean': 0.0, 'stddev': 0.05}\n",
    "activation_dens = 'relu'\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1014, 128)         8832      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1008, 256)         229632    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 336, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 330, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 106, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 104, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 102, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,450,498\n",
      "Trainable params: 11,450,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "# https://medium.com/@romannempyre/sentiment-analysis-using-1d-convolutional-neural-networks-part-1-f8b6316489a2\n",
    "# https://github.com/chaitjo/character-level-cnn\n",
    "# https://missinglink.ai/guides/deep-learning-frameworks/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
    "# https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=5c4a324fc922\n",
    "# https://medium.com/@bramblexu/character-level-cnn-with-keras-50391c3adf33\n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "#model.add(layers.Input(shape=(max_len_text,),  dtype='int32'))\n",
    "\n",
    "# AS the one hot approach, leave us with a very sparse and high dimensional matrix, we apply embedding\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "# Embedding ( \n",
    "#    max_features/input_dim = we got just 70 different types of inputs / Size of the vocabulary, \n",
    "#    output_dim = learn N dimensional embeddings for each of the input_dim words/characters,\n",
    "#    input_length needed to flatten = cut the words/characters of each element to that lenght so the max\n",
    "#                                     qty of elements in each phrase will be that one\n",
    "# )    \n",
    "model.add(layers.Embedding(max_features_for_embedding, output_from_embedding, input_length=max_len_text))\n",
    "# After the Embedding layer,\n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "# Output: (batch dimension, input_length, output_dim)\n",
    "\n",
    "# Convolutional Layer 1D\n",
    "# In text \n",
    "# Conv1D(\n",
    "#    filters = dimensionality of the output space\n",
    "#    kernel_size = window_size\n",
    "#    stride = position jumps of the window (defaults to 1)\n",
    "# )\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv, kernel_initializer=initializer_large_model))\n",
    "\n",
    "# Max Pooling 1D\n",
    "# MaxPool1D(\n",
    "#    pool_size = window size\n",
    "#    strides = by default equals pool_size, so each window doesn't overlap\n",
    "# )\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "\n",
    "# Second part, full connected layers\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation=activation_dens, kernel_initializer=initializer_large_model))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(1024, activation=activation_dens))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "# Last layer, according to problem to solve\n",
    "# (just have 2 classes)\n",
    "# But, it has to be though a Dense(2,...) , with Dense(1,...) not working so neither do activation \"sigmoid\"\n",
    "model.add(layers.Dense(2, activation='softmax')) \n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer and loss function and metrics to return \n",
    "# binary_crossentropy ? \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlphabetDict(alpha):\n",
    "    d = {}; c = 1\n",
    "    for a in alpha: d[a] = c; c += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndClasses(tokenizer, info, alpha, max_len_text, batch_len = None, from_i = 0):\n",
    "        \n",
    "    lines = info['lines']\n",
    "    labels = info['labels']\n",
    "    if batch_len != None: \n",
    "        lines = lines[from_i:(from_i + batch_len)]\n",
    "        labels = labels[from_i:(from_i + batch_len)]\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    \"\"\"EX: [[15, 9, 2, 5, 3, 1, 13, 12, 28, 1, 14, 17 ...\"\"\" # Each, number represents a letter in the alphabet\n",
    "    data = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len_text, padding='post')\n",
    "    # Data of shape (quantity of info (sentences), max_len_text)\n",
    "    \"\"\"EX: array([[15,  9,  2, ...,  0,  0,  0],\n",
    "       [ 6,  8,  2, ...,  0,  0,  0],\n",
    "       [21,  5,  3, ...,  0,  0,  0]], dtype=int32)\"\"\"  \n",
    "    \n",
    "    classes = utils.to_categorical(labels, num_classes=len(info['total_labels']))\n",
    "    \"\"\"EX: array([[0., 1.],\n",
    "       [0., 1.],\n",
    "       [1., 0.]], dtype=float32)\"\"\"\n",
    "    \n",
    "    return {'data': data, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runByBatches(model, train_info, test_info, alpha, max_len_text, batch_size, test_batch_size):\n",
    "    \n",
    "    # Preprocess Data \n",
    "    tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=alpha) # , oov_token= ?  \n",
    "    tokenizer.word_index = getAlphabetDict(alpha)\n",
    "    \"\"\"EX: {'a': 1,\n",
    "         'b': 2,\n",
    "         'c': 3 ... \"\"\"\n",
    "    \n",
    "    # After each epoch , the test data is validated\n",
    "    #test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text)\n",
    "        \n",
    "    total_train_len = train_info['total_lines']\n",
    "    total_tst_len = test_info['total_lines']\n",
    "    count = count_tst = 0\n",
    "    hist_arr = []\n",
    "    \n",
    "    while count < total_train_len:\n",
    "        \n",
    "        # Get elements\n",
    "        size = batch_size if batch_size < (total_train_len - count) else total_train_len - count\n",
    "        size_tst = test_batch_size if test_batch_size < (total_tst_len - count_tst) else total_tst_len - count_tst\n",
    "        train_data = getDataAndClasses(tokenizer, train_info, alpha, max_len_text, size, count)\n",
    "        test_data = getDataAndClasses(tokenizer, test_info, alpha, max_len_text, size_tst, count_tst)\n",
    "        print(f'doing from trining/test: {count} - {size} / {count_tst} - {size_tst}')\n",
    "        count += batch_size   \n",
    "        count_tst += test_batch_size\n",
    "        \n",
    "        history = model.fit(train_data['data'], \n",
    "                            train_data['classes'],\n",
    "                            validation_data=(test_data['data'], test_data['classes']),\n",
    "                            epochs=5, # 5000\n",
    "                            batch_size=128\n",
    "                           )\n",
    "        hist_arr.append(history)\n",
    "        \n",
    "        train_data = None\n",
    "        test_data = None\n",
    "        \n",
    "    return hist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWindowOfBatchInArray(a, from_i, to_i):\n",
    "    tot_len = len(a)\n",
    "    until_pos = from_i + to_i\n",
    "    remaining = 0\n",
    "    if until_pos > tot_len: \n",
    "        remaining = until_pos - tot_len \n",
    "        return a[from_i:(tot_len + 1)]+a[0:remaining]\n",
    "    else:\n",
    "        return a[from_i:to_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing from trining/test: 0 - 400000 / 0 - 44445\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 400000 samples, validate on 44445 samples\n",
      "Epoch 1/5\n",
      "400000/400000 [==============================] - 2958s 7ms/step - loss: 0.3375 - acc: 0.8451 - val_loss: 0.2392 - val_acc: 0.9045\n",
      "Epoch 2/5\n",
      "399872/400000 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9145"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-79ab8a6f64a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mmax_len_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mtest_data_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                        )\n",
      "\u001b[0;32m<ipython-input-9-493b9e582017>\u001b[0m in \u001b[0;36mrunByBatches\u001b[0;34m(model, train_info, test_info, alpha, max_len_text, batch_size, test_batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                            )\n\u001b[1;32m     35\u001b[0m         \u001b[0mhist_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_batch_size = math.ceil(test_data_info['total_lines']/\\\n",
    "    (train_data_info['total_lines']/test_data_info['total_lines']))\n",
    "\n",
    "hist_arr = runByBatches(model, \n",
    "                        train_data_info, \n",
    "                        test_data_info, \n",
    "                        alphabet, \n",
    "                        max_len_text, \n",
    "                        test_data_info['total_lines'],\n",
    "                        test_batch_size\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
