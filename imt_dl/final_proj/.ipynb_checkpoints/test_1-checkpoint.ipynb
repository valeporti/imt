{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import initializers , preprocessing, utils\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}\n",
      "\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Alphabet\n",
    "# \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "numbers = \"0123456789\"\n",
    "other_char = \",;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\" # original: -,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}, changed: -,;.!?:’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\n",
    "new_line = \"\\n\"\n",
    "\n",
    "alphabet = letters + numbers + other_char + new_line\n",
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text\n",
    "\n",
    "text = \"hel o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Quantization\n",
    "\n",
    "# [70 rows (the alphabet), text lenght columns]\n",
    "\n",
    "def quantize_text(alphabet, text):\n",
    "    results = np.zeros((len(alphabet), len(text)))\n",
    "    for i, char in enumerate(text):\n",
    "        if char.lower() in alphabet:\n",
    "            results[alphabet.index(char.lower()), i] = 1\n",
    "    return results\n",
    "\n",
    "quantized_text = quantize_text(alphabet, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(quantized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of text\n",
    "# __label__2 This is my first PDA/Organizer: I purchased this about 4 months ago and it really is easy to use especially if you are familiar with Microsoft Word and Excel. I use it to copy files from my PC that I want to have handy. The only thing I wish it had is a backlight. But that hasn't come into play too often. I usually have to charge it every 2 or 3 days. I recommend this product if you are not worried about having a lot accessories to go with it, because they are not that many available.\n",
    "# __label__1 Piece of Crap: I have ordered thousands of items in my lifetime, and bar none, this is the biggest piece of crap I have ever received. It is supposed to come completely put together, but when it arrived, there were no less than 10 little parts that had come off the screen assembly and one screen that was loose and all of its parts dangling. The first thing that happenned is that I cut myself on one of the screens because the protective side had come off. I was bleeding for quite some time. The second thing I noticed was the extremely poor quality of the material and assembly. I am very handy by nature, but even I had to give up trying to put this piece of junk back together again. Plus it was not worth it, even if I had put it together again, all I would have then had was a piece of crap fireplace screen.\n",
    "# data from  https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "\n",
    "dir_name = './data/'\n",
    "test_file_name = 'test.ft.txt'\n",
    "train_file_name = 'train.ft.txt'\n",
    "\n",
    "\n",
    "def getListOfLabelsAndLinesFromFile(file_name): # label 1 = 0, label 2 = 1\n",
    "    \n",
    "    labels = []\n",
    "    lines = []\n",
    "    total_labels = []\n",
    "    count = 0\n",
    "    file = open(file_name, 'r')\n",
    "    for line in file:\n",
    "        match = re.match('(__label__([0-9])) (.*)', line)\n",
    "        if match:\n",
    "            label = int(match.group(2)) - 1 # to make the label start in 0 (ZERO)\n",
    "            text = match.group(3)\n",
    "            if not label in total_labels: total_labels.append(label)\n",
    "            labels.append(label)\n",
    "            lines.append(text)\n",
    "        else:\n",
    "            print('---- ERROR ----')\n",
    "        count += 1\n",
    "    file.close()\n",
    "    return { 'labels': labels, 'lines': lines, 'total_labels': total_labels, 'total_lines': count }\n",
    "\n",
    "train_data_info = getListOfLabelsAndLinesFromFile(dir_name + train_file_name)\n",
    "test_data_info = getListOfLabelsAndLinesFromFile(dir_name + test_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/valeporti/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1014, 128)         8960      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1008, 256)         229632    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 336, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 330, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 106, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 104, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 102, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 11,449,601\n",
      "Trainable params: 11,449,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "# https://medium.com/@romannempyre/sentiment-analysis-using-1d-convolutional-neural-networks-part-1-f8b6316489a2\n",
    "# https://github.com/chaitjo/character-level-cnn\n",
    "# https://missinglink.ai/guides/deep-learning-frameworks/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
    "# https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=5c4a324fc922\n",
    "# https://medium.com/@bramblexu/character-level-cnn-with-keras-50391c3adf33\n",
    "\n",
    "# Embedding\n",
    "max_len_text = 1014\n",
    "max_features_for_embedding = len(alphabet)\n",
    "output_from_embedding = 128\n",
    "\n",
    "# Large Model\n",
    "initializer_large = { 'mean': 0.0, 'stddev': 0.02}\n",
    "activation_conv = 'relu'\n",
    "conv_output_dim = 256\n",
    "pool_size = 3\n",
    "kernel_size_7 = 7 # first_WITH_maxpool\n",
    "kernel_size_3 = 3 # intermediate_NO_maxpool\n",
    "\n",
    "# Small model\n",
    "initializer_small = { 'mean': 0.0, 'stddev': 0.05}\n",
    "activation_dens = 'relu'\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# AS the one hot approach, leave us with a very sparse and high dimensional matrix, we apply embedding\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "# Embedding ( \n",
    "#    max_features/input_dim = we got just 70 different types of inputs / Size of the vocabulary, \n",
    "#    output_dim = learn N dimensional embeddings for each of the input_dim words/characters,\n",
    "#    input_length needed to flatten = cut the words/characters of each element to that lenght so the max\n",
    "#                                     qty of elements in each phrase will be that one\n",
    "# )    \n",
    "model.add(layers.Embedding(max_features_for_embedding, output_from_embedding, input_length=max_len_text))\n",
    "# After the Embedding layer,\n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "# Output: (batch dimension, input_length, output_dim)\n",
    "\n",
    "# Convolutional Layer 1D\n",
    "# In text \n",
    "# Conv1D(\n",
    "#    filters = dimensionality of the output space\n",
    "#    kernel_size = window_size\n",
    "#    stride = position jumps of the window (defaults to 1)\n",
    "# )\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv, kernel_initializer=initializer_large_model))\n",
    "\n",
    "# Max Pooling 1D\n",
    "# MaxPool1D(\n",
    "#    pool_size = window size\n",
    "#    strides = by default equals pool_size, so each window doesn't overlap\n",
    "# )\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_7, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.Conv1D(filters=conv_output_dim, kernel_size=kernel_size_3, activation=activation_conv))\n",
    "model.add(layers.MaxPool1D(pool_size=pool_size))\n",
    "\n",
    "# Second part, full connected layers\n",
    "initializer_large_model = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation=activation_dens, kernel_initializer=initializer_large_model))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(1024, activation=activation_dens))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "# Last layer, according to problem to solve\n",
    "# (just have 2 classes)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer and loss function and metrics to return \n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7f953bc5a048>\n"
     ]
    }
   ],
   "source": [
    "# treat the data before passing it to the model\n",
    "# train_data_info \n",
    "# test_data_info \n",
    "# { 'labels': , 'lines': , 'total_labels': , 'total_lines':  }\n",
    "\n",
    "small_file_name = 'small.txt'\n",
    "small_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=True, filters=None) # , oov_token= ?  \n",
    "tokenizer.fit_on_texts(alphabet)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#one_hot_results = tokenizer.texts_to_matrix(train_data_info['lines'], mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ',': 37,\n",
       " ';': 38,\n",
       " '.': 39,\n",
       " '!': 40,\n",
       " '?': 41,\n",
       " ':': 42,\n",
       " \"'\": 43,\n",
       " '\"': 44,\n",
       " '/': 45,\n",
       " '\\\\': 46,\n",
       " '|': 47,\n",
       " '_': 48,\n",
       " '@': 49,\n",
       " '#': 50,\n",
       " '$': 51,\n",
       " '%': 52,\n",
       " '^': 53,\n",
       " '&': 54,\n",
       " '*': 55,\n",
       " '~': 56,\n",
       " '`': 57,\n",
       " '+': 58,\n",
       " '-': 59,\n",
       " '=': 60,\n",
       " '<': 61,\n",
       " '>': 62,\n",
       " '(': 63,\n",
       " ')': 64,\n",
       " '[': 65,\n",
       " ']': 66,\n",
       " '{': 67,\n",
       " '}': 68,\n",
       " '\\n': 69}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsmall_file_name = 'small.txt'\\nsmall_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\\n\\ndef getAllLinesQuantized(lines, alphabet):\\n    quantized_text = []\\n    for line in lines:\\n        quantized = quantize_text(alphabet, line)\\n        quantized_text.append(quantized)\\n    return quantized_text\\n\\nquantized_text = getAllLinesQuantized(small_data_info['lines'], final_string)\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize my way\n",
    "# AVOID.. A LOT OF MEMORY!!!\n",
    "\"\"\"\n",
    "small_file_name = 'small.txt'\n",
    "small_data_info = getListOfLabelsAndLinesFromFile(dir_name + small_file_name)\n",
    "\n",
    "def getAllLinesQuantized(lines, alphabet):\n",
    "    quantized_text = []\n",
    "    for line in lines:\n",
    "        quantized = quantize_text(alphabet, line)\n",
    "        quantized_text.append(quantized)\n",
    "    return quantized_text\n",
    "\n",
    "quantized_text = getAllLinesQuantized(small_data_info['lines'], final_string)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 18, 5, 1, 20, 3, 4, 42, 13, 25, 12, 15, 22, 5, 12, 25, 16, 1, 20, 8, 1, 19, 15, 14, 5, 15, 6, 20, 8, 5, 7, 18, 5, 1, 20, 22, 15, 9, 3, 5, 19, 15, 6, 8, 5, 18, 7, 5, 14, 5, 18, 1, 20, 9, 15, 14, 39, 9, 8, 1, 22, 5, 12, 9, 19, 20, 5, 14, 5, 4, 20, 15, 20, 8, 9, 19, 3, 4, 6, 15, 18, 25, 5, 1, 18, 19, 1, 14, 4, 9, 19, 20, 9, 12, 12, 12, 15, 22, 5, 9, 20, 39, 23, 8, 5, 14, 9, 43, 13, 9, 14, 1, 7, 15, 15, 4, 13, 15, 15, 4, 9, 20, 13, 1, 11, 5, 19, 13, 5, 6, 5, 5, 12, 2, 5, 20, 20, 5, 18, 39, 1, 2, 1, 4, 13, 15, 15, 4, 10, 21, 19, 20, 5, 22, 1, 16, 15, 18, 1, 20, 5, 19, 12, 9, 11, 5, 19, 21, 7, 1, 18, 9, 14, 20, 8, 5, 18, 1, 9, 14, 39, 20, 8, 9, 19, 3, 4, 10, 21, 19, 20, 15, 15, 26, 5, 19, 12, 9, 6, 5, 39, 22, 15, 3, 1, 12, 19, 1, 18, 5, 10, 21, 19, 1, 20, 19, 20, 21, 21, 14, 14, 9, 14, 7, 1, 14, 4, 12, 25, 18, 9, 3, 19, 10, 21, 19, 20, 11, 9, 12, 12, 39, 15, 14, 5, 15, 6, 12, 9, 6, 5, 43, 19, 8, 9, 4, 4, 5, 14, 7, 5, 13, 19, 39, 20, 8, 9, 19, 9, 19, 1, 4, 5, 19, 5, 18, 20, 9, 19, 12, 5, 3, 4, 9, 14, 13, 25, 2, 15, 15, 11, 39, 23, 8, 25, 19, 8, 5, 14, 5, 22, 5, 18, 13, 1, 4, 5, 9, 20, 2, 9, 7, 9, 19, 10, 21, 19, 20, 2, 5, 25, 15, 14, 4, 13, 5, 39, 5, 22, 5, 18, 25, 20, 9, 13, 5, 9, 16, 12, 1, 25, 20, 8, 9, 19, 37, 14, 15, 13, 1, 20, 20, 5, 18, 2, 12, 1, 3, 11, 37, 23, 8, 9, 20, 5, 37, 25, 15, 21, 14, 7, 37, 15, 12, 4, 37, 13, 1, 12, 5, 37, 6, 5, 13, 1, 12, 5, 5, 22, 5, 18, 25, 2, 15, 4, 25, 19, 1, 25, 19, 15, 14, 5, 20, 8, 9, 14, 7, 44, 23, 8, 15, 23, 1, 19, 20, 8, 1, 20, 19, 9, 14, 7, 9, 14, 7, 41, 44], [15, 14, 5, 15, 6, 20, 8, 5, 2, 5, 19, 20, 7, 1, 13, 5, 13, 21, 19, 9, 3, 19, 15, 21, 14, 4, 20, 18, 1, 3, 11, 19, 59, 6, 15, 18, 1, 7, 1, 13, 5, 9, 4, 9, 4, 14, 43, 20, 18, 5, 1, 12, 12, 25, 16, 12, 1, 25, 42, 4, 5, 19, 16, 9, 20, 5, 20, 8, 5, 6, 1, 3, 20, 20, 8, 1, 20, 9, 8, 1, 22, 5, 15, 14, 12, 25, 16, 12, 1, 25, 5, 4, 1, 19, 13, 1, 12, 12, 16, 15, 18, 20, 9, 15, 14, 15, 6, 20, 8, 5, 7, 1, 13, 5, 37, 20, 8, 5, 13, 21, 19, 9, 3, 9, 8, 5, 1, 18, 4, 63, 16, 12, 21, 19, 20, 8, 5, 3, 15, 14, 14, 5, 3, 20, 9, 15, 14, 20, 15, 3, 8, 18, 15, 14, 15, 20, 18, 9, 7, 7, 5, 18, 23, 8, 9, 3, 8, 23, 1, 19, 7, 18, 5, 1, 20, 1, 19, 23, 5, 12, 12, 64, 12, 5, 4, 13, 5, 20, 15, 16, 21, 18, 3, 8, 1, 19, 5, 20, 8, 5, 19, 15, 21, 14, 4, 20, 18, 1, 3, 11, 37, 1, 14, 4, 9, 20, 18, 5, 13, 1, 9, 14, 19, 15, 14, 5, 15, 6, 13, 25, 6, 1, 22, 15, 18, 9, 20, 5, 1, 12, 2, 21, 13, 19, 39, 20, 8, 5, 18, 5, 9, 19, 1, 14, 9, 14, 3, 18, 5, 4, 9, 2, 12, 5, 13, 9, 24, 15, 6, 6, 21, 14, 37, 5, 16, 9, 3, 37, 1, 14, 4, 5, 13, 15, 20, 9, 15, 14, 1, 12, 19, 15, 14, 7, 19, 39, 20, 8, 15, 19, 5, 19, 1, 4, 1, 14, 4, 2, 5, 1, 21, 20, 9, 6, 21, 12, 20, 18, 1, 3, 11, 19, 9, 5, 19, 16, 5, 3, 9, 1, 12, 12, 25, 12, 9, 11, 5, 37, 1, 19, 20, 8, 5, 18, 5, 43, 19, 14, 15, 20, 20, 15, 15, 13, 1, 14, 25, 15, 6, 20, 8, 15, 19, 5, 11, 9, 14, 4, 19, 15, 6, 19, 15, 14, 7, 19, 9, 14, 13, 25, 15, 20, 8, 5, 18, 22, 9, 4, 5, 15, 7, 1, 13, 5, 19, 15, 21, 14, 4, 20, 18, 1, 3, 11, 19, 39, 9, 13, 21, 19, 20, 1, 4, 13, 9, 20, 20, 8, 1, 20, 15, 14, 5, 15, 6, 20, 8, 5, 19, 15, 14, 7, 19, 63, 12, 9, 6, 5, 59, 1, 4, 9, 19, 20, 1, 14, 20, 16, 18, 15, 13, 9, 19, 5, 64, 8, 1, 19, 2, 18, 15, 21, 7, 8, 20, 20, 5, 1, 18, 19, 20, 15, 13, 25, 5, 25, 5, 19, 15, 14, 13, 1, 14, 25, 15, 3, 3, 1, 19, 9, 15, 14, 19, 39, 13, 25, 15, 14, 5, 3, 15, 13, 16, 12, 1, 9, 14, 20, 1, 2, 15, 21, 20, 20, 8, 9, 19, 19, 15, 21, 14, 4, 20, 18, 1, 3, 11, 9, 19, 20, 8, 1, 20, 20, 8, 5, 25, 21, 19, 5, 7, 21, 9, 20, 1, 18, 6, 18, 5, 20, 20, 9, 14, 7, 5, 6, 6, 5, 3, 20, 19, 9, 14, 13, 1, 14, 25, 15, 6, 20, 8, 5, 19, 15, 14, 7, 19, 37, 23, 8, 9, 3, 8, 9, 6, 9, 14, 4, 4, 9, 19, 20, 18, 1, 3, 20, 9, 14, 7, 39, 2, 21, 20, 5, 22, 5, 14, 9, 6, 20, 8, 15, 19, 5, 23, 5, 18, 5, 14, 43, 20, 9, 14, 3, 12, 21, 4, 5, 4, 9, 23, 15, 21, 12, 4, 19, 20, 9, 12, 12, 3, 15, 14, 19, 9, 4, 5, 18, 20, 8, 5, 3, 15, 12, 12, 5, 3, 20, 9, 15, 14, 23, 15, 18, 20, 8, 9, 20, 39], [2, 1, 20, 20, 5, 18, 9, 5, 19, 4, 9, 5, 4, 23, 9, 20, 8, 9, 14, 1, 25, 5, 1, 18, 39, 39, 39, 42, 9, 2, 15, 21, 7, 8, 20, 20, 8, 9, 19, 3, 8, 1, 18, 7, 5, 18, 9, 14, 10, 21, 12, 29, 27, 27, 30, 1, 14, 4, 9, 20, 23, 15, 18, 11, 5, 4, 15, 11, 6, 15, 18, 1, 23, 8, 9, 12, 5, 39, 20, 8, 5, 4, 5, 19, 9, 7, 14, 9, 19, 14, 9, 3, 5, 1, 14, 4, 3, 15, 14, 22, 5, 14, 9, 5, 14, 20, 39, 8, 15, 23, 5, 22, 5, 18, 37, 1, 6, 20, 5, 18, 1, 2, 15, 21, 20, 1, 25, 5, 1, 18, 37, 20, 8, 5, 2, 1, 20, 20, 5, 18, 9, 5, 19, 23, 15, 21, 12, 4, 14, 15, 20, 8, 15, 12, 4, 1, 3, 8, 1, 18, 7, 5, 39, 13, 9, 7, 8, 20, 1, 19, 23, 5, 12, 12, 10, 21, 19, 20, 7, 5, 20, 1, 12, 11, 1, 12, 9, 14, 5, 4, 9, 19, 16, 15, 19, 1, 2, 12, 5, 19, 37, 15, 18, 12, 15, 15, 11, 5, 12, 19, 5, 23, 8, 5, 18, 5, 6, 15, 18, 1, 3, 8, 1, 18, 7, 5, 18, 20, 8, 1, 20, 3, 15, 13, 5, 19, 23, 9, 20, 8, 2, 1, 20, 20, 5, 18, 9, 5, 19, 20, 8, 1, 20, 8, 1, 22, 5, 2, 5, 20, 20, 5, 18, 19, 20, 1, 25, 9, 14, 7, 16, 15, 23, 5, 18, 39]]\n"
     ]
    }
   ],
   "source": [
    "# Convert string to index\n",
    "sequences = tokenizer.texts_to_sequences(small_data_info['lines'])\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1014)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because text have different length, \n",
    "# we have to make all text as the same length, so the CNN can handle the batch data.\n",
    "# so that max we got 1014 and if the text is inferior, it will be filled with zeroes\n",
    "# we use \"padding post so that the zeroes goes to the end, and not at the beggining\"\n",
    "data = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len_text, padding='post')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_info['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_info['total_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = utils.to_categorical(small_data_info['labels'], num_classes=len(small_data_info['total_labels']))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train and Test Data Sets\n",
    "\n",
    "# Convert string to index\n",
    "sequences_train = tokenizer.texts_to_sequences(train_data_info['lines'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data_info['lines'])\n",
    "# Padding\n",
    "data_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=max_len_text, padding='post')\n",
    "data_test = preprocessing.sequence.pad_sequences(sequences_test, maxlen=max_len_text, padding='post')\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes indexation\n",
    "train_classes = utils.to_categorical(train_data_info['labels'], num_classes=len(train_data_info['total_labels']))\n",
    "test_classes = utils.to_categorical(test_data_info['labels'], num_classes=len(test_data_info['total_labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "history = model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
