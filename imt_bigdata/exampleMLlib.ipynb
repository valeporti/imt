{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example de Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+-----------+-------+--------+\n",
      "|origin|model|hour|temperature|arrival|freezing|\n",
      "+------+-----+----+-----------+-------+--------+\n",
      "|   SFO| B737|  18|       95.1|   late|     1.0|\n",
      "|   SEA| A319|   5|       65.7| ontime|     1.0|\n",
      "|   LAX| B747|  15|       31.5|   late|     0.0|\n",
      "|   ATL| A319|  14|       40.5|   late|     1.0|\n",
      "+------+-----+----+-----------+-------+--------+\n",
      "\n",
      "+-----------+--------+\n",
      "|temperature|freezing|\n",
      "+-----------+--------+\n",
      "|       95.1|     1.0|\n",
      "|       65.7|     1.0|\n",
      "|       31.5|     0.0|\n",
      "|       40.5|     1.0|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "arrival_data = [origin: string, model: string ... 3 more fields]\n",
       "binarizer = binarizer_b49743b287ab\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "binarizer_b49743b287ab"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Binarizer\n",
    "\n",
    "val arrival_data = spark.createDataFrame(Seq((\"SFO\", \"B737\", 18, 95.1, \"late\"), (\"SEA\", \"A319\", 5, 65.7, \"ontime\"),\n",
    "(\"LAX\", \"B747\", 15, 31.5, \"late\"), (\"ATL\", \"A319\", 14, 40.5, \"late\") )) .toDF(\"origin\", \"model\", \"hour\", \"temperature\", \"arrival\")\n",
    "\n",
    "// seuillage de temperature et création d'une colonne de sortie\n",
    "//définition de la transformation\n",
    "val binarizer = new Binarizer().setInputCol(\"temperature\")\n",
    "                               .setOutputCol(\"freezing\")\n",
    "                               .setThreshold(35.6)\n",
    "\n",
    "binarizer.transform(arrival_data).show\n",
    "\n",
    "// show the current values of the parameters in binarizer transformer\n",
    "binarizer.explainParams\n",
    "\n",
    "//inputCol: input column name (current: temperature)\n",
    "//outputCol: output column name (default: binarizer_60430bb4e97f__output, current: freezing)\n",
    "//threshold: threshold used to binarize continuous features (default: 0.0, current: 35.6)\n",
    "\n",
    "// show the transformation result\n",
    "// avec selection des colonnes temperature et freezing\n",
    "binarizer.transform(arrival_data).select(\"temperature\", \"freezing\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seuillage à plusieurs niveaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]+-----------+---------+\n",
      "|temperature|intensity|\n",
      "+-----------+---------+\n",
      "|       31.5|      0.0|\n",
      "|       40.5|      1.0|\n",
      "|       65.7|      1.0|\n",
      "|       95.1|      2.0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bucketBorders = Array(-1.0, 32.0, 70.0, 150.0)\n",
       "bucketer = bucketizer_114084ae720c\n",
       "output = [origin: string, model: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[origin: string, model: string ... 4 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val bucketBorders = Array(-1.0, 32.0, 70.0, 150.0)\n",
    "\n",
    "val bucketer = new Bucketizer().setSplits(bucketBorders)\n",
    "                               .setInputCol(\"temperature\")\n",
    "                               .setOutputCol(\"intensity\")\n",
    "val output = bucketer.transform(arrival_data)\n",
    "output.select(\"temperature\", \"intensity\")\n",
    "          .orderBy(\"temperature\")\n",
    "          .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------+-------------+\n",
      "|user|      major|majorIdx|    majorVect|\n",
      "+----+-----------+--------+-------------+\n",
      "|John|       Math|       3|(7,[3],[1.0])|\n",
      "|Mary|Engineering|       2|(7,[2],[1.0])|\n",
      "|Jeff| Philosophy|       7|    (7,[],[])|\n",
      "|Jane|       Math|       3|(7,[3],[1.0])|\n",
      "|Lyna|    Nursing|       4|(7,[4],[1.0])|\n",
      "+----+-----------+--------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "student_major_data = [user: string, major: string ... 1 more field]\n",
       "oneHotEncoder = oneHot_f8da1cbec062\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "oneHot_f8da1cbec062"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "val student_major_data = spark.createDataFrame(Seq((\"John\", \"Math\", 3),\n",
    "                                              (\"Mary\", \"Engineering\", 2),\n",
    "                                              (\"Jeff\", \"Philosophy\", 7),\n",
    "                                              (\"Jane\", \"Math\", 3),\n",
    "                                              (\"Lyna\", \"Nursing\", 4) ))\n",
    "                                              .toDF(\"user\", \"major\",\n",
    "                                              \"majorIdx\")\n",
    "// majorVect est un sparse vector\n",
    "// la valeur max n'a pas de colonne\n",
    "// majorIdx varie de 0 à 7 (sauf si préciser)\n",
    "// valeur de 0 à 6 : 7 colonnes valeur 7 : pas de colonne\n",
    "val oneHotEncoder = new OneHotEncoder().setInputCol(\"majorIdx\")\n",
    "                                       .setOutputCol(\"majorVect\")\n",
    "oneHotEncoder.transform(student_major_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+------+\n",
      "|words                                                                   |tokens|\n",
      "+------------------------------------------------------------------------+------+\n",
      "|[spark, spark, is, a, unified, data, analytics, engine]                 |8     |\n",
      "|[it, is, fun, to, work, with, spark]                                    |7     |\n",
      "|[there, is, a, lot, of, exciting, sessions, at, upcoming, spark, summit]|11    |\n",
      "|[mllib, transformer, estimator, evaluator, and, pipelines]              |6     |\n",
      "+------------------------------------------------------------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text_data = [id: int, line: string]\n",
       "tokenizer = tok_12313160b45b\n",
       "tokenized = [id: int, line: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, line: string ... 1 more field]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.sql.functions._\n",
    "val text_data = spark.createDataFrame(Seq(\n",
    "                                     (1, \"Spark spark is a unified data analytics engine\"),\n",
    "                                     (2, \"It is fun to work with Spark\"),\n",
    "                                     (3, \"There is a lot of exciting sessions at upcoming Spark summit\"),\n",
    "                                     (4, \"mllib transformer estimator evaluator and pipelines\")  )\n",
    "                         ).toDF(\"id\", \"line\")\n",
    "val tokenizer = new Tokenizer().setInputCol(\"line\").setOutputCol(\"words\")\n",
    "val tokenized = tokenizer.transform(text_data)\n",
    "tokenized.select(\"words\").withColumn(\"tokens\", size(col(\"words\"))).\n",
    "show(false)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "|words                                                                   |filtered                                             |\n",
      "+------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "|[spark, spark, is, a, unified, data, analytics, engine]                 |[spark, spark, unified, data, analytics, engine]     |\n",
      "|[it, is, fun, to, work, with, spark]                                    |[fun, work, spark]                                   |\n",
      "|[there, is, a, lot, of, exciting, sessions, at, upcoming, spark, summit]|[lot, exciting, sessions, upcoming, spark, summit]   |\n",
      "|[mllib, transformer, estimator, evaluator, and, pipelines]              |[mllib, transformer, estimator, evaluator, pipelines]|\n",
      "+------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enStopWords = Array(i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "val enStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "// enStopWords contients les mots à supprimer\n",
    "// words colonne qui contient les mots à filtrer\n",
    "// filtered colonne qui contient le résultat\n",
    "val remover = new StopWordsRemover().setStopWords(enStopWords)\n",
    "                                    .setInputCol(\"words\")\n",
    "                                    .setOutputCol(\"filtered\")\n",
    "// use the tokenized from Listing 8-5 example\n",
    "val cleanedTokens = remover.transform(tokenized)\n",
    "cleanedTokens.select(\"words\",\"filtered\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing\n",
    "\n",
    "a chaque mot est associé un nombre\n",
    "on compte l'occurence de chaque mot (dans la ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+---------------------------------------------------------------+\n",
      "|filtered                                             |TFOut                                                          |\n",
      "+-----------------------------------------------------+---------------------------------------------------------------+\n",
      "|[spark, spark, unified, data, analytics, engine]     |(4096,[991,1185,1461,3377,3717],[1.0,2.0,1.0,1.0,1.0])         |\n",
      "|[fun, work, spark]                                   |(4096,[251,1185,1575],[1.0,1.0,1.0])                           |\n",
      "|[lot, exciting, sessions, upcoming, spark, summit]   |(4096,[724,1185,1255,1962,2966,3023],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[mllib, transformer, estimator, evaluator, pipelines]|(4096,[994,2132,2697,3522,3894],[1.0,1.0,1.0,1.0,1.0])         |\n",
      "+-----------------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tf = hashingTF_3cd9525647dc\n",
       "tfResult = [id: int, line: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, line: string ... 3 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.HashingTF\n",
    "val tf = new HashingTF().setInputCol(\"filtered\")\n",
    "                        .setOutputCol(\"TFOut\")\n",
    "                        .setNumFeatures(4096)\n",
    "val tfResult = tf.transform(cleanedTokens)\n",
    "tfResult.select(\"filtered\", \"TFOut\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example où les caractéristiques sont sauvés sous la forme de vector avec des doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------+---------------+\n",
      "|hour|temperature|on_time|       features|\n",
      "+----+-----------+-------+---------------+\n",
      "|  18|       95.1|   true|[18.0,95.1,1.0]|\n",
      "|   5|       65.7|   true| [5.0,65.7,1.0]|\n",
      "|  15|       31.5|  false|[15.0,31.5,0.0]|\n",
      "|  14|       40.5|  false|[14.0,40.5,0.0]|\n",
      "+----+-----------+-------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "arrival_features = [hour: int, temperature: double ... 1 more field]\n",
       "assembler = vecAssembler_4845635224ad\n",
       "output = [hour: int, temperature: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[hour: int, temperature: double ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "val arrival_features  = spark.createDataFrame(Seq(\n",
    "                                               (18, 95.1, true),\n",
    "                                               (5, 65.7, true), (15, 31.5,\n",
    "                                               false),\n",
    "                                               (14, 40.5, false) ))\n",
    "                                            .toDF(\"hour\", \"temperature\",\n",
    "                                            \"on_time\")\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"hour\",\n",
    "\"temperature\", \"on_time\"))\n",
    "                                     .setOutputCol(\"features\")\n",
    "val output = assembler.transform(arrival_features)\n",
    "output.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+-----------+-------+-------------------------------------+-----+\n",
      "|origin|model|hour|temperature|arrival|features                             |label|\n",
      "+------+-----+----+-----------+-------+-------------------------------------+-----+\n",
      "|SFO   |B737 |18  |95.1       |late   |(8,[0,5,6,7],[1.0,18.0,95.1,1711.8]) |0.0  |\n",
      "|SEA   |A319 |5   |65.7       |ontime |[0.0,0.0,1.0,1.0,0.0,5.0,65.7,328.5] |1.0  |\n",
      "|LAX   |B747 |15  |31.5       |late   |(8,[4,5,6,7],[1.0,15.0,31.5,472.5])  |0.0  |\n",
      "|ATL   |A319 |14  |40.5       |late   |[0.0,1.0,0.0,1.0,0.0,14.0,40.5,567.0]|0.0  |\n",
      "+------+-----+----+-----------+-------+-------------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "arrival_data = [origin: string, model: string ... 3 more fields]\n",
       "formula = RFormula(arrival ~ . + hour:temperature) (uid=rFormula_8349a1b200bd)\n",
       "output = [origin: string, model: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[origin: string, model: string ... 5 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RFormula\n",
    "val arrival_data = spark.createDataFrame(Seq((\"SFO\", \"B737\", 18, 95.1, \"late\"),\n",
    "(\"SEA\", \"A319\", 5, 65.7, \"ontime\"), (\"LAX\", \"B747\", 15, 31.5, \"late\"), (\"ATL\", \"A319\", 14, 40.5, \"late\") )) .toDF(\"origin\", \"model\", \"hour\", \"temperature\", \"arrival\")\n",
    "// arrival est le label\n",
    "// on prend les caractéristiques on ajoute hour*temperature\n",
    "// par la création du vector feature\n",
    "// les doubles restent doublent\n",
    "// les valeurs discretees sont hotencoder\n",
    "// la colonne arrival est \n",
    "val formula = new RFormula().setFormula(\"arrival ~ . + hour:temperature\")\n",
    "                            .setFeaturesCol(\"features\")\n",
    "                            .setLabelCol(\"label\")\n",
    "// call fit function first, which returns a model (type of transformer), then call transform\n",
    "// attention au mélange vector dense et vector sparse\n",
    "val output = formula.fit(arrival_data).transform(arrival_data)\n",
    "output.select(\"*\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example TFDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                   |features                                                                                                                                                                                                                                                                         |\n",
      "+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[spark, is, a, unified, data, analytics, engine]                        |(4096,[991,1185,1461,2130,3377,3601,3717],[0.9162907318741551,0.22314355131420976,0.9162907318741551,0.5108256237659907,0.9162907318741551,0.22314355131420976,0.9162907318741551])                                                                                              |\n",
      "|[spark, is, cool, and, it, is, fun, to, work, with, spark]              |(4096,[159,244,251,1185,1565,1575,2435,3586,3601],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.44628710262841953,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.44628710262841953])                                                |\n",
      "|[there, is, a, lot, of, exciting, sessions, at, upcoming, spark, summit]|(4096,[311,724,836,1185,1255,1447,1962,2130,2966,3023,3601],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.22314355131420976,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.22314355131420976])|\n",
      "|[mllib, transformer, estimator, evaluator, and, pipelines]              |(4096,[994,1565,2132,2697,3522,3894],[0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.9162907318741551])                                                                                                                        |\n",
      "+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- line: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordFreqVect: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|wordFreqVect                                                                                              |features                                                                                                                                                                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(4096,[991,1185,1461,2130,3377,3601,3717],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                  |(4096,[991,1185,1461,2130,3377,3601,3717],[0.9162907318741551,0.22314355131420976,0.9162907318741551,0.5108256237659907,0.9162907318741551,0.22314355131420976,0.9162907318741551])                                                                                              |\n",
      "|(4096,[159,244,251,1185,1565,1575,2435,3586,3601],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0])                  |(4096,[159,244,251,1185,1565,1575,2435,3586,3601],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.44628710262841953,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.44628710262841953])                                                |\n",
      "|(4096,[311,724,836,1185,1255,1447,1962,2130,2966,3023,3601],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(4096,[311,724,836,1185,1255,1447,1962,2130,2966,3023,3601],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.22314355131420976,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.22314355131420976])|\n",
      "|(4096,[994,1565,2132,2697,3522,3894],[1.0,1.0,1.0,1.0,1.0,1.0])                                           |(4096,[994,1565,2132,2697,3522,3894],[0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.9162907318741551])                                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text_data = [id: int, line: string]\n",
       "tokenizer = tok_2135c5547bc0\n",
       "tf = hashingTF_0b7bcd542cc2\n",
       "tfResult = [id: int, line: string ... 2 more fields]\n",
       "idf = idf_cc6983da6d17\n",
       "idfModel = idf_cc6983da6d17\n",
       "weightedWords = [id: int, line: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, line: string ... 3 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.feature.HashingTF\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "// texte 4 lignes\n",
    "val text_data = spark.createDataFrame(Seq(\n",
    "                                     (1, \"Spark is a unified data analytics engine\"),\n",
    "                                     (2, \"Spark is cool and it is fun to work with Spark\"),\n",
    "                                     (3, \"There is a lot of exciting sessions at upcoming Spark summit\"),\n",
    "                                     (4, \"mllib transformer estimator evaluator and pipelines\")  ))\n",
    "                                   .toDF(\"id\", \"line\")\n",
    "// tokenize de chaque line\n",
    "val tokenizer = new Tokenizer().setInputCol(\"line\")\n",
    "                               .setOutputCol(\"words\")\n",
    "// the output column of the Tokenizer transformer is the input to HashingTF\n",
    "// hash\n",
    "val tf = new HashingTF().setInputCol(\"words\")\n",
    "                        .setOutputCol(\"wordFreqVect\")\n",
    "                        .setNumFeatures(4096)\n",
    "val tfResult = tf.transform(tokenizer.transform(text_data))\n",
    "// the output of the HashingTF transformer is the input to IDF estimator\n",
    "val idf = new IDF().setInputCol(\"wordFreqVect\")\n",
    "                   .setOutputCol(\"features\")\n",
    "// since IDF is an estimator, call the fit function\n",
    "val idfModel = idf.fit(tfResult)\n",
    "// the returned object is a Model, which is of type Transformer\n",
    "val weightedWords = idfModel.transform(tfResult)\n",
    "\n",
    "weightedWords.select(\"words\", \"features\").show(false)\n",
    "\n",
    "weightedWords.printSchema\n",
    "// attention résultat en mode vector sparse\n",
    "// les mots sont \"des doubles\"\n",
    "// the feature column contains a vector for the weight of each word, since it is long, the output is not included //below\n",
    "weightedWords.select(\"wordFreqVect\", \"features\").show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id| genre|genreIdx|\n",
      "+---+------+--------+\n",
      "|  1|Comedy|     0.0|\n",
      "|  6|Comedy|     0.0|\n",
      "|  3|Comedy|     0.0|\n",
      "|  5|Action|     1.0|\n",
      "|  2|Action|     1.0|\n",
      "|  4|Horror|     2.0|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movie_data = [id: int, genre: string]\n",
       "movieIndexer = strIdx_d3aafb9a72fc\n",
       "movieIndexModel = strIdx_d3aafb9a72fc\n",
       "indexedMovie = [id: int, genre: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, genre: string ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "val movie_data = spark.createDataFrame(Seq(\n",
    "                                              (1, \"Comedy\"),\n",
    "                                              (2, \"Action\"),\n",
    "                                              (3, \"Comedy\"),\n",
    "                                              (4, \"Horror\"),\n",
    "                                              (5, \"Action\"),\n",
    "                                              (6, \"Comedy\")  )\n",
    "                                     ).toDF(\"id\", \"genre\")\n",
    "val movieIndexer = new StringIndexer().setInputCol(\"genre\")\n",
    "                                      .setOutputCol(\"genreIdx\")\n",
    "// first fit the data\n",
    "val movieIndexModel = movieIndexer.fit(movie_data)\n",
    "// use returned transformer to transform the data\n",
    "val indexedMovie = movieIndexModel.transform(movie_data)\n",
    "indexedMovie.orderBy(\"genreIdx\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example oneHotEncoder (définition - model - application model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+--------------+\n",
      "| id| genre|genreIdx|genreIdxVector|\n",
      "+---+------+--------+--------------+\n",
      "|  2|Action|     1.0| (2,[1],[1.0])|\n",
      "|  5|Action|     1.0| (2,[1],[1.0])|\n",
      "|  3|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  6|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  1|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  4|Horror|     2.0|     (2,[],[])|\n",
      "+---+------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oneHotEncoderEst = oneHotEncoder_a67e6d8e8674\n",
       "oneHotEncoderModel = oneHotEncoder_a67e6d8e8674\n",
       "oneHotEncoderVect = [id: int, genre: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, genre: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "// the input column genreIdx is the output column of StringIndex in listing 8-9\n",
    "val oneHotEncoderEst = new OneHotEncoderEstimator().setInputCols(Array(\"genreIdx\"))\n",
    "                                   .setOutputCols(Array(\"genreIdxVector\"))\n",
    "// fit the indexedMovie data produced in listing 8-10\n",
    "val oneHotEncoderModel = oneHotEncoderEst.fit(indexedMovie)\n",
    "val oneHotEncoderVect = oneHotEncoderModel.transform(indexedMovie)\n",
    "oneHotEncoderVect .orderBy(\"genre\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-------------------------------------------------------------------+\n",
      "|word                                     |feature                                                            |\n",
      "+-----------------------------------------+-------------------------------------------------------------------+\n",
      "|[Unified, data, analytics, engine, Spark]|[-0.04857720620930195,-0.039790508151054386,-0.0047628857195377355]|\n",
      "|[People, use, Hive, for, data, analytics]|[-0.019269779634972412,-0.0019863341003656387,0.04896292210711787] |\n",
      "|[MapReduce, is, not, fading, away]       |[0.09048619866371155,0.02390633299946785,0.004982998222112656]     |\n",
      "+-----------------------------------------+-------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "documentDF = [word: array<string>]\n",
       "word2Vec = w2v_7365afe6ef06\n",
       "model = w2v_7365afe6ef06\n",
       "result = [word: array<string>, feature: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: array<string>, feature: vector]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "val documentDF = spark.createDataFrame(Seq(\n",
    "                                   \"Unified data analytics engine Spark\".\n",
    "                                   split(\" \"),\n",
    "                                   \"People use Hive for data analytics\".\n",
    "                                   split(\" \"),\"MapReduce is not fading away\".split(\" \") ).map(Tuple1.apply)).toDF(\"word\")\n",
    "// initialisation\n",
    "val word2Vec = new Word2Vec().setInputCol(\"word\")\n",
    "                             .setOutputCol(\"feature\") .setVectorSize(3)\n",
    "                             .setMinCount(0)\n",
    "// recherche model\n",
    "val model = word2Vec.fit(documentDF)\n",
    "// application model\n",
    "val result = model.transform(documentDF)\n",
    "result.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     word|        similarity|\n",
      "+---------+------------------+\n",
      "|   engine|0.9133241772651672|\n",
      "|MapReduce|0.7623026967048645|\n",
      "|     Hive|0.7179173827171326|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// find similar words to Spark, the result shows both Hive and MapReduce are similar.\n",
    "model.findSynonyms(\"Spark\", 3).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|  word|         similarity|\n",
      "+------+-------------------+\n",
      "| Spark| 0.7179174423217773|\n",
      "|fading| 0.5859972238540649|\n",
      "|engine|0.43200281262397766|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// find similar words to Hive, the result shows Spark is similar\n",
    "model.findSynonyms(\"Hive\", 3).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.0,5.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employee_data = [empId: int, features: vector]\n",
       "minMaxScaler = minMaxScal_c9c0173078dc\n",
       "scalerModel = minMaxScal_c9c0173078dc\n",
       "scaledData = [empId: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[empId: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MinMaxScaler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "val employee_data = spark.createDataFrame(Seq(\n",
    "                                    (1, Vectors.dense(125400, 5.3)),\n",
    "                                    (2, Vectors.dense(179100, 6.9)),\n",
    "                                    (3, Vectors.dense(154770, 5.2)),\n",
    "                                    (4, Vectors.dense(199650, 4.11))))\n",
    "                                  .toDF(\"empId\", \"features\")\n",
    "val minMaxScaler = new MinMaxScaler().setMin(0.0)\n",
    "                                     .setMax(5.0)\n",
    "                                     .setInputCol(\"features\")\n",
    "                                     .setOutputCol(\"scaledFeatures\")\n",
    "val scalerModel = minMaxScaler.fit(employee_data)\n",
    "val scaledData = scalerModel.transform(employee_data)\n",
    "println(s\"Features scaled to range: [${minMaxScaler.getMin},${minMaxScaler.getMax}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------+\n",
      "|features       |scaledFeatures                         |\n",
      "+---------------+---------------------------------------+\n",
      "|[125400.0,5.3] |[0.0,2.1326164874551963]               |\n",
      "|[179100.0,6.9] |[3.616161616161616,5.0]                |\n",
      "|[154770.0,5.2] |[1.9777777777777779,1.9534050179211468]|\n",
      "|[199650.0,4.11]|[5.0,0.0]                              |\n",
      "+---------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledFeatures\").show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------------------------------+\n",
      "|empId|features       |scaledFeatures                            |\n",
      "+-----+---------------+------------------------------------------+\n",
      "|1    |[125400.0,5.3] |[-1.2290717420781212,-0.06743742573177663]|\n",
      "|2    |[179100.0,6.9] |[0.4490658767775897,1.3248191055048923]   |\n",
      "|3    |[154770.0,5.2] |[-0.3112523404805006,-0.15445345893406812]|\n",
      "|4    |[199650.0,4.11]|[1.091258205781032,-1.1029282208390485]   |\n",
      "+-----+---------------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employee_data = [empId: int, features: vector]\n",
       "standardScaler = stdScal_4b7eb83adfc1\n",
       "standardMode = stdScal_4b7eb83adfc1\n",
       "standardData = [empId: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[empId: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "val employee_data = spark.createDataFrame(Seq(\n",
    "                                         (1, Vectors.dense(125400, 5.3)),\n",
    "                                         (2, Vectors.dense(179100, 6.9)),\n",
    "                                         (3, Vectors.dense(154770, 5.2)),\n",
    "                                         (4, Vectors.dense(199650, 4.11))))\n",
    "                                    .toDF(\"empId\", \"features\")\n",
    "// set the unit standard deviation to true and center around the mean\n",
    "val standardScaler = new StandardScaler().setWithStd(true)\n",
    "                                         .setWithMean(true)                                         .setInputCol(\"features\")\n",
    "                                         .setOutputCol(\"scaledFeatures\")\n",
    "val standardMode = standardScaler.fit(employee_data)\n",
    "val standardData = standardMode.transform(employee_data)\n",
    "standardData.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_data = [id: int, line: string ... 1 more field]\n",
       "tokenizer = tok_ffd4311802bf\n",
       "hashingTF = hashingTF_1d4db814d98c\n",
       "logisticReg = logreg_5f2970095a9a\n",
       "pipeline = pipeline_324d8d59bf80\n",
       "logisticRegModel = pipeline_324d8d59bf80\n",
       "prevModel = pipeline_324d8d59bf80\n",
       "prevPipeline = pipeli...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeli..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "\n",
    "val text_data = spark.createDataFrame(Seq(\n",
    "                                  (1, \"Spark is a unified data analytics engine\", 0.0),\n",
    "                                  (2, \"Spark is cool and it is fun to work with Spark\", 0.0),\n",
    "                                  (3, \"There is a lot of exciting sessions at upcoming Spark summit\", 0.0),\n",
    "                                  (4, \"signup to win a million dollars\", 0.0)  )\n",
    "                                ).toDF(\"id\", \"line\", \"label\")\n",
    "val tokenizer = new Tokenizer().setInputCol(\"line\").setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol)\n",
    "                               .setOutputCol(\"features\")\n",
    "                               .setNumFeatures(4096)\n",
    "val logisticReg = new LogisticRegression().setMaxIter(5).setRegParam(0.01)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, logisticReg))\n",
    "\n",
    "val logisticRegModel = pipeline.fit(text_data)\n",
    "\n",
    "// persist model and pipeline\n",
    "logisticRegModel.write.overwrite().save(\"tmp/spark-logistic-regression-model\")\n",
    "pipeline.write.overwrite().save(\"tmp/logistic-regression-pipeline\")\n",
    "//logisticRegModel.save(\"/Users/lecornu/tmp\")\n",
    "// load model and pipeline\n",
    "val prevModel = PipelineModel.load(\"tmp/spark-logistic-regression-model\")\n",
    "val prevPipeline = Pipeline.load(\"tmp/logistic-regression-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:26: error: not found: value sparkConf\n",
       "       sparkConf\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@61cb1242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
