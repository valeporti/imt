{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example de calcul de Corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "|   [6.0,7.0,1.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n",
      "Pearson correlation matrix:\n",
      " 1.0                  0.12367498475994064  0.15161960871578067  0.41287206669597387  \n",
      "0.12367498475994064  1.0                  0.5019646441105268   0.9357441262904027   \n",
      "0.15161960871578067  0.5019646441105268   1.0                  0.5598852584152163   \n",
      "0.41287206669597387  0.9357441262904027   0.5598852584152163   1.0                  \n",
      "Spearman correlation matrix:\n",
      " 1.0                  0.16222142113076254  0.18136906252750293  0.36842105263157904  \n",
      "0.16222142113076254  1.0                  0.5590169943749475   0.9733285267845753   \n",
      "0.18136906252750293  0.5590169943749475   1.0                  0.5441071875825088   \n",
      "0.36842105263157904  0.9733285267845753   0.5441071875825088   1.0                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List((4,[0,3],[1.0,-2.0]), [4.0,5.0,0.0,3.0], [6.0,7.0,0.0,8.0], [6.0,7.0,1.0,8.0], (4,[0,3],[9.0,1.0]))\n",
       "df = [features: vector]\n",
       "coeff1 = \n",
       "coeff2 = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0                  0.12367498475994064  0.15161960871578067  0.41287206669597387\n",
       "0.12367498475994064  1.0                  0.5019646441105268   0.9357441262904027\n",
       "0.15161960871578067  0.5019646441105268   1.0                  0.5598852584152163\n",
       "0.41287206669597387  0.9357441262904027   0.5598852584152163   1.0\n",
       "1.0                  0.16222142113076254  0.181369062527502...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\n",
    "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
    "  Vectors.dense(6.0, 7.0, 1.0, 8.0),\n",
    "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
    ")\n",
    "// dense : ensemble des variables du vecteur\n",
    "// sparse : seulement les valeurs non nulles (nbre de valeur puis  couples (indice, valeur))\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "\n",
    "df.show()\n",
    "// par defaut pearson correlation\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(s\"Pearson correlation matrix:\\n $coeff1\")\n",
    "\n",
    "// autre choix spearman\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(s\"Spearman correlation matrix:\\n $coeff2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple test d'hypothèse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues = [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom [2,3]\n",
      "statistics [0.75,1.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List((0.0,[0.5,10.0]), (0.0,[1.5,20.0]), (1.0,[1.5,30.0]), (0.0,[3.5,30.0]), (0.0,[3.5,40.0]), (1.0,[3.5,40.0]))\n",
       "df = [label: double, features: vector]\n",
       "chi = [[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "//  Attention : label et features sont categorical (nbre finis de valeurs)\n",
    "// label 2 classes, features 3 classes (resp. 4)\n",
    "\n",
    "// le test est fait pour chaque feature vs label\n",
    "// le degré de liberté est calculé automatiquement\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, 40.0))\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"label\", \"features\")\n",
    "val chi = ChiSquareTest.test(df, \"features\", \"label\").head\n",
    "println(s\"pValues = ${chi.getAs[Vector](0)}\")\n",
    "println(s\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\")}\")\n",
    "println(s\"statistics ${chi.getAs[Vector](2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé statistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with weight: mean = [3.333333333333333,5.0,6.333333333333333], variance = [2.0,4.5,2.0]\n",
      "without weight: mean = [3.0,4.5,6.0], sum = [2.0,4.5,2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List(([2.0,3.0,5.0],1.0), ([4.0,6.0,7.0],2.0))\n",
       "df = [features: vector, weight: double]\n",
       "meanVal = [3.333333333333333,5.0,6.333333333333333]\n",
       "varianceVal = [2.0,4.5,2.0]\n",
       "meanVal2 = [3.0,4.5,6.0]\n",
       "varianceVal2 = [2.0,4.5,2.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2.0,4.5,2.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.Summarizer\n",
    "\n",
    "val data = Seq(\n",
    "  (Vectors.dense(2.0, 3.0, 5.0), 1.0),\n",
    "  (Vectors.dense(4.0, 6.0, 7.0), 2.0)\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"features\", \"weight\")\n",
    "\n",
    "// metrics possible : mean, variance, count, numNonZeros, max, min, normL1, normL2\n",
    "\n",
    "// calcul incluant le poids\n",
    "val (meanVal, varianceVal) = df.select(Summarizer.metrics(\"mean\", \"variance\")\n",
    "                                       .summary($\"features\", $\"weight\").as(\"summary\"))\n",
    "                                       .select(\"summary.mean\", \"summary.variance\")\n",
    "                                       .as[(Vector, Vector)].first()\n",
    "\n",
    "println(s\"with weight: mean = ${meanVal}, variance = ${varianceVal}\")\n",
    "\n",
    "// calcul sans tenir compte du poids\n",
    "val (meanVal2, varianceVal2) = df.select(Summarizer.mean($\"features\"), Summarizer.variance($\"features\"))\n",
    "  .as[(Vector, Vector)].first()\n",
    "\n",
    "println(s\"without weight: mean = ${meanVal2}, sum = ${varianceVal2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n",
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_8376c45f74b7-aggregationDepth: 2,\n",
      "\tlogreg_8376c45f74b7-elasticNetParam: 0.0,\n",
      "\tlogreg_8376c45f74b7-family: auto,\n",
      "\tlogreg_8376c45f74b7-featuresCol: features,\n",
      "\tlogreg_8376c45f74b7-fitIntercept: true,\n",
      "\tlogreg_8376c45f74b7-labelCol: label,\n",
      "\tlogreg_8376c45f74b7-maxIter: 10,\n",
      "\tlogreg_8376c45f74b7-predictionCol: prediction,\n",
      "\tlogreg_8376c45f74b7-probabilityCol: probability,\n",
      "\tlogreg_8376c45f74b7-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_8376c45f74b7-regParam: 0.01,\n",
      "\tlogreg_8376c45f74b7-standardization: true,\n",
      "\tlogreg_8376c45f74b7-threshold: 0.5,\n",
      "\tlogreg_8376c45f74b7-tol: 1.0E-6\n",
      "}\n",
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_8376c45f74b7-aggregationDepth: 2,\n",
      "\tlogreg_8376c45f74b7-elasticNetParam: 0.0,\n",
      "\tlogreg_8376c45f74b7-family: auto,\n",
      "\tlogreg_8376c45f74b7-featuresCol: features,\n",
      "\tlogreg_8376c45f74b7-fitIntercept: true,\n",
      "\tlogreg_8376c45f74b7-labelCol: label,\n",
      "\tlogreg_8376c45f74b7-maxIter: 30,\n",
      "\tlogreg_8376c45f74b7-predictionCol: prediction,\n",
      "\tlogreg_8376c45f74b7-probabilityCol: myProbability,\n",
      "\tlogreg_8376c45f74b7-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_8376c45f74b7-regParam: 0.1,\n",
      "\tlogreg_8376c45f74b7-standardization: true,\n",
      "\tlogreg_8376c45f74b7-threshold: 0.55,\n",
      "\tlogreg_8376c45f74b7-tol: 1.0E-6\n",
      "}\n",
      "([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171034065,0.9429269582896593], prediction=1.0\n",
      "([3.0,2.0,-0.1], 0.0) -> prob=[0.9238522311704118,0.07614776882958815], prediction=0.0\n",
      "([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114779739,0.8902722388522026], prediction=1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training = [label: double, features: vector]\n",
       "lr = logreg_8376c45f74b7\n",
       "model1 = LogisticRegressionModel: uid = logreg_8376c45f74b7, numClasses = 2, numFeatures = 3\n",
       "paramMap = \n",
       "paramMap2 = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tlogreg_8376c45f74b7-maxIter: 30,\n",
       "\tlogreg_8376c45f74b7-regParam: 0.1,\n",
       "\tlogreg_8376c45f74b7-threshold: 0.55\n",
       "}\n",
       "{\n",
       "\tlogreg_8376c45f74b7-probabilityCol: myProbability\n",
       "}\n",
       "paramMapCombined: org.apache.spark.ml.p...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training data from a list of (label, features) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Create a LogisticRegression instance. This instance is an Estimator.\n",
    "val lr = new LogisticRegression()\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")\n",
    "\n",
    "// We may set parameters using setter methods.\n",
    "lr.setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "\n",
    "// Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "val model1 = lr.fit(training)\n",
    "// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "// LogisticRegression instance.\n",
    "println(s\"Model 1 was fit using parameters: ${model1.parent.extractParamMap}\")\n",
    "\n",
    "// We may alternatively specify parameters using a ParamMap,\n",
    "// which supports several methods for specifying parameters.\n",
    "val paramMap = ParamMap(lr.maxIter -> 20)\n",
    "  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.\n",
    "  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.\n",
    "\n",
    "// One can also combine ParamMaps.\n",
    "val paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\")  // Change output column name.\n",
    "val paramMapCombined = paramMap ++ paramMap2\n",
    "\n",
    "// Now learn a new model using the paramMapCombined parameters.\n",
    "// paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "val model2 = lr.fit(training, paramMapCombined)\n",
    "println(s\"Model 2 was fit using parameters: ${model2.parent.extractParamMap}\")\n",
    "\n",
    "// Prepare test data.\n",
    "val test = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Make predictions on test data using the Transformer.transform() method.\n",
    "// LogisticRegression.transform will only use the 'features' column.\n",
    "// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    "// 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "model2.transform(test)\n",
    "  .select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    "  .collect()\n",
    "  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.15964077387874118,0.8403592261212589], prediction=1.0\n",
      "(5, l m n) --> prob=[0.8378325685476612,0.16216743145233875], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976273,0.9307336686702373], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.9821575333444208,0.01784246665557917], prediction=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training = [id: bigint, text: string ... 1 more field]\n",
       "tokenizer = tok_8c192e7394b8\n",
       "hashingTF = hashingTF_38ffd2141876\n",
       "lr = logreg_574e2c90ae6a\n",
       "pipeline = pipeline_ecddaccc9a07\n",
       "model = pipeline_ecddaccc9a07\n",
       "sameModel = pipeline_ecddaccc9a07\n",
       "test = [id: bi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bi..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training documents from a list of (id, text, label) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF()\n",
    "  .setNumFeatures(1000)\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.001)\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = spark.createDataFrame(Seq(\n",
    "  (4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"spark hadoop spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test)\n",
    "  .select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "  .collect()\n",
    "  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
